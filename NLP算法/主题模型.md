## 主题模型

- NLP-机器学习笔试面试题解析 [Github链接](https://github.com/WerterHong/Machine-Learning-Algorithm-NLP/)
- **主题模型** (若公式显示错误，请点击此链接) [有道云笔记](http://note.youdao.com/noteshare?id=c2de80f4ed4a254a5926c846f66789db&sub=AE3D2D6112064F3BAC1BBDA64627400B)

### 1. 背景知识

> ==`Gamma`函数、`Beta`分布、`Dirichlet`分布推导见==：[LDA数学八卦](http://www.52nlp.cn/lda-math-%e8%ae%a4%e8%af%86betadirichlet%e5%88%86%e5%b8%831)。

#### 1.0 Gamma 函数

当`n`为整数时，`n`的阶乘定义为`n!=n *(n-1) *(n-2) * ...  * 2 * 1`。当`n`不是整数时，`n`的阶乘是多少？

`Gamma`函数的基本公式如下：

```math
\Gamma(x)=\int_{0}^{\infty} t^{x-1} e^{-t} d t
```
Gamma函数图像如下：

<p align="center">
<img src="https://note.youdao.com/yws/public/resource/b68e61b9fa9321cee59f807071b5bfc5/4ABE7DAC8BDA4CF0B334FEDD1C69B131?ynotemdtimestamp=1564394744465" />
</p>

<p align="center">
<strong>Fig</strong>. Gamma 函数
</p>

Gamma函数有如下性质：
1. ==递归性质==（利用分步积分法可证）

```math
\Gamma(x+1)=x \Gamma(x)
```

2. 可表述为==阶乘在实数集上的拓展==

```math
\Gamma(n)=(n-1) !
```

> Q: 为何定义`Γ`函数的时候，不使得这个函数的定义满足`Γ(n)=n!`而是`Γ(n)=(n−1)!` ？
>
> A: 将`Gamma`函数定义中的`$t^{x−1}$`替换为`$t^x$`，即
```math
\Gamma(x)=\int_{0}^{\infty} t^{x} e^{-t} d t
```
> 可得`$\Gamma(n)=n !$`
>
> 定义如下积分（==`Beta`函数==）
```math
B(m, n)=\int_{0}^{1} x^{m-1}(1-x)^{n-1} d x
```
> 如果满足`$\Gamma(n)=(n-1) !$`，那么有
```math
B(m, n)=\frac{\Gamma(m) \Gamma(n)}{\Gamma(m+n)}
```
> 如果满足`$\Gamma(n)=n !$`，令
```math
E(m, n)=\int_{0}^{1} x^{m}(1-x)^{n} d x
```
> 则有
```math
E(m, n)=\frac{\Gamma(m) \Gamma(n)}{\Gamma(m+n+1)}
```
> 不如`B(m,n)`优美。



#### 1.1 二项分布

**伯努利分布**，又称两点分布或`0-1`分布，是一个==离散型的随机分布==，其中的随机变量只有两类取值，非正即负`{+，-}`。

**二项分布**即==重复`n`次的伯努利试验==，记为`X~(b(n,p))`。二项分布的概率密度函数为：

```math
P(K=k)=\left(\begin{array}{l}{n} \\ {k}\end{array}\right) p^{k}(1-p)^{n-k} = \frac{n!}{k!(n-k)!} \times p^{k}(1-p)^{n-k}, \quad k=0,1,...,n
```
其中`$\left(\begin{array}{l}{n} \\ {k}\end{array}\right)=\frac{n!}{k!(n-k)!}$`是==二项系数==，又记为`C(n,k)`。

#### 1.2 多项分布

**多项分布**是二项分布扩展到多维的情况，是指单次试验中的随机变量取值不再是`0-1`的，而是有==多种离散值的可能==`(1,2,3,...,k)`。比如投掷骰子实验，`N`次实验结果服从`k=6`的多项分布。其中

```math
\sum_{i=1}^{k} p_{i}=1, p_{i}>0
```
多项分布的概率密度函数为：

```math
P\left(x_{1}, x_{2}, \ldots, x_{k} ; n, p_{1}, p_{2}, \ldots, p_{k}\right)=\frac{n !}{x_{1} ! \cdots x_{k} !} p_{1}^{x_{1}} \cdots p_{k}^{x_{k}}
```

>  **共轭先验分布**：在贝叶斯概率理论中，如果后验概率`P(θ|x)`和先验概率`p(θ)`满足==同样的分布律==，那么，先验分布和后验分布被叫做==共轭分布==，同时，先验分布叫做==似然函数的共轭先验分布==。

#### 1.3 Beta 分布

`Beta`分布是指一组分布在`[0,1]`区间的==连续概率分布==，其中参数为`α>0`，`β>0`，概率密度函数为：

```math
f(x ; \alpha, \beta)=\frac{1}{B(\alpha, \beta)} x^{\alpha-1}(1-x)^{\beta-1}
```
其中

```math
\frac{1}{B(\alpha, \beta)}=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha) \Gamma(\beta)}, \quad \Gamma(x)=\int_{0}^{\infty} t^{x-1} e^{-t} d t
```

`α,β`不同取值的`Beta`分布如下图：

<p align="center">
<img src="https://note.youdao.com/yws/public/resource/b68e61b9fa9321cee59f807071b5bfc5/E7904C86A9424647BEACD8647B5CFBA8?ynotemdtimestamp=1564394744465" width=500 />
</p>

<p align="center">
<strong>Fig</strong>. 不同取值的Beta分布
</p>

> 如果二项分布的参数`p`选取的先验分布是`Beta`分布，那么以`p`为参数的二项分布用==贝叶斯估计==得到的后验分布仍然服从`Beta`分布，那么`Beta`分布就是==二项分布的共轭先验分布==(`Beta-Binomial`共轭)，用数学公式表述就是：

```math
\operatorname{Beta}(p | \alpha, \beta)+\operatorname{BinomCount}\left(m_{1}, m_{2}\right)=\operatorname{Beta}\left(p | \alpha+m_{1}, \beta+m_{2}\right)
```
- 如果`p∼Beta(t|α,β)`, `Beta`分布的==期望==为：

```math
E(p)=\frac{\alpha}{\alpha+\beta}
```

#### 1.4 Dirichlet 分布

**Dirichlet 分布**是`Beta`分布在高纬度上的推广。 Dirichlet分布的的密度函数为：

```math
f\left(x_{1}, x_{2}, \ldots, x_{k} ; \alpha_{1}, \alpha_{2}, \ldots, \alpha_{k}\right)=\frac{1}{B(\alpha)} \prod_{i=1}^{k} x_{i}^{\alpha^{i}-1}
```
其中

```math
B(\alpha)=\frac{\prod_{i=1}^{k} \Gamma\left(\alpha^{i}\right)}{\Gamma\left(\sum_{i=1}^{k} \alpha^{i}\right)}, \quad \sum_{i=1}^{k} x^{i}=1
```

不同`α`下的`Dirichlet`分布如下图：

<p align="center">
<img src="https://note.youdao.com/yws/public/resource/b68e61b9fa9321cee59f807071b5bfc5/79E5EDD91E7A42B69722F5007EC20B70?ynotemdtimestamp=1564394744465" width=500 />
</p>

<p align="center">
<strong>Fig</strong>. 不同 α 下的 Dirichlet 分布
</p>

> 且类比于Beta分布，Dirichlet也是==多项分布的共轭先验分布==(`Dirichlet-Multinomial`共轭)：

```math
Dirichlet(\vec{p} | \vec{\alpha})+MultiCount(\vec{m})=Dirichlet(\vec{p} | \vec{\alpha}+\vec{m})
```
- `Dirichlet`分布的==期望==如下：

```math
E(p)=\left(\frac{\alpha^{1}}{\sum_{i=1}^{K} \alpha_{i}}, \frac{\alpha^{2}}{\sum_{i=1}^{K} \alpha_{i}}, \cdots, \frac{\alpha^{K}}{\sum_{i=1}^{K} \alpha_{i}}\right)
```
其**物理意义**也就是每个参数的估计值是==其对应事件的先验的参数和数据中的计数的和==在整体计数中的比例。

#### 1.5 分布之间的联系

> - 二项分布和多项分布很相似，`Beta`分布和`Dirichlet`分布很相似
>
> - `Beta`分布是二项式分布的共轭先验概率分布，而`Dirichlet`分布是多项式分布的共轭先验概率分布

### 2. `Unigram Model`

**文本模型**(`Unigram Model`)假设文本中的词服从==多项式(`Multinomial`)分布==，多项式分布的先验分布为`Dirichlet`分布。

对于文档`$w=\{w_1,w_2,...,w_N\}$`，用`$p(w_n)$`表示词`$w_n$`的先验概率，生成文档`w`的概率为：

```math
p(\mathbf{w})=\prod_{n=1}^{N} p\left(w_{n}\right)
```
其图模型为(图中被涂色的`w`表示可观测变量，`N`表示一篇文档中总共`N`个单词，`M`表示`M`篇文档):

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/b68e61b9fa9321cee59f807071b5bfc5/C29F2636898F4029A17D83F9DB50A8AD?ynotemdtimestamp=1564577368141" />
    <br/>
    <strong>Fig</strong>. 文本(Unigram)模型
</p>

或为：

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/b68e61b9fa9321cee59f807071b5bfc5/9EF9EFF60D4C4B6C8BC2EE5CF23C7332?ynotemdtimestamp=1564577368141" height=400 />
    <br/>
    <strong>Fig</strong>. 概率图模型
</p>

上图中的`$w_n$`表示在文本中观察到的第`n`个词，`$n \in [1,N]$`表示该文本中一共有N个单词。加上方框表示重复，即一共有`N`个这样的随机变量。其中，`p`和`α`是隐含未知变量：

- `p`是词服从的`Multinomial`分布的参数
- `α`是Dirichlet分布（即`Multinomial`分布的先验分布）的参数。

一般`α`由==经验事先给定==，`p`由观察到的文本中出现的词学习得到，表示==文本中出现每个词的概率==。

此时该==文本的生成概率==就等于：

```math
p(\vec{w} | \vec{\alpha})=\int p(\vec{w_n} | \vec{p}) \cdot p(\vec{p} | \vec{\alpha}) d \vec{p}
```
推到计算可得：

```math
p(\vec{w} | \vec{\alpha})=\frac{\Delta(\vec{n}+\vec{\alpha})}{\Delta(\vec{\alpha})}
```

### 3. `pLSA` 模型

`pLSA`模型(`probabilistic Latent Semantic Analysis`，**概率隐语义分析**)是用一个==生成模型==来建模文章的生成过程，增加了主题模型，形成简单的贝叶斯网络，可以用`EM`==算法==学习模型参数。`pLSA`模型有三个假设：
1. 一篇**文章**可以由多个**主题**构成
2. 每一个**主题**由一组**词**的概率分布来表示
3. 一篇**文章**中的每一个具体的**词**都来自于一个固定的**主题**

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/b68e61b9fa9321cee59f807071b5bfc5/6B4BCA99BC3A46E58324991033494EFD?ynotemdtimestamp=1564577368141" width=500 />
    <br/>
    <strong>Fig</strong>. pLSA模型示意图
</p>

**参数定义**：
- `$P(d_i)$`表示海量文档中某篇文档被选中的过程
- `$P(w_j|d_i)$`表示词`$w_j$`在给定文档`$d_i$`中出现的概率
- `$P(z_k|d_i)$`表示具体某个主题`$z_k$`在给定文档`$d_i$`下出现的概率
- `$P(w_j|z_k)$`表示具体某个词`$w_j$`在给定主题`$z_k$`下出现的概率，与主题关系越密切的词，其条件概率`$P(w_j|z_k)$`越大

根据定义可以得到“==文档-词项==”==的生成模型==：

1. 按照概率`$P(d_i)$`选一篇文档`$d_i$`
2. 选定文档`$d_i$`后，从主题分布`$P(z_k|d_i)$`中按照概率选择一个隐含的主题类别`$z_k$`
3. 选定主题`$z_k$`后，从词分布`$P(w_j|z_k)$`中按照概率选择一个词`$w_j$`

`pLSA`模型如下图所示（图中被涂色的`d`、`w`表示可观测变量，未被涂色的`z`表示未知的==隐变量==，`N`表示一篇文档中总共`N`个单词，`M`表示`M`篇文档）：

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/b68e61b9fa9321cee59f807071b5bfc5/F81F2DF6134A4CE1A356BD478ECFDECF?ynotemdtimestamp=1564577368141" />
    <br/>
    <strong>Fig</strong>. pLSA 模型
</p>

根据大量已知的**文档-词项`$P(w_j|d_i)$`信息**，训练出文档-主题`$P(z_k|d_i)$`和主题-词项`$P(w_j|z_k)$`，如下公式所示：

```math
P\left(w_{j} | d_{i}\right)=\sum_{k=1}^{K} P\left(w_{j} | z_{k}\right) P\left(z_{k} | d_{i}\right)
```
故得到**文档中每个词的生成概率**为：

```math
\begin{aligned} P\left(d_{i}, w_{j}\right) &=P\left(d_{i}\right) P\left(w_{j} | d_{i}\right) \\ &=P\left(d_{i}\right) \sum_{k=1}^{K} P\left(w_{j} | z_{k}\right) P\left(z_{k} | d_{i}\right) \end{aligned}
```
 由于`$P(d_i)$`可事先计算求出，==而`$P(w_j|z_k)$`和`$P(z_k|d_i)$`未知，所以`$θ=P(w_j|z_k)P(z_k|d_i)$`就是我们要估计的参数（值）==，通俗点说，就是要==最大化这个`θ`==。待估计的参数中含有==隐变量`z`==，所以考虑`EM`算法 [(有道云笔记 Link)](http://note.youdao.com/noteshare?id=fefebccc430e697e257cda8c603fca90&sub=8CFF2C512DBC4AA29FC01DC0742B91E5)。

 - #### `EM` 算法估计 `pLSA` 的参数

 [见博客](https://blog.csdn.net/pipisorry/article/details/42560877)

### 4. `LDA` 模型

`LDA`(`Latent Dirichlet Allocation`)就是在`pLSA`的基础上加层==贝叶斯框架(考虑先验知识)==，即`LDA`就是`pLSA`的贝叶斯版本。`LDA`==根据给定的一篇文档，反推其主题分布==。

`LDA`模型如下图所示(图中只有词`$w_{m,n}$`是已知量，`ϕ`表示词分布，`Θ`表示主题分布，`α`是主题分布`Θ`的先验分布（即`Dirichlet`分布）的参数，`β`是词分布`ϕ`的先验分布（即`Dirichlet`分布）的参数，`N`表示文档的单词总数，`M`表示文档的总数):

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/b68e61b9fa9321cee59f807071b5bfc5/011901010A794251BFB1B9ACD55991F9?ynotemdtimestamp=1564581225297" height=300 />
    <br/>
    <strong>Fig</strong>. LDA 模型
</p>

`LDA`模型的==文档生成模型(主题分布跟词分布由`Dirichlet`先验随机确定==):

1. 按照先验概率`$P(d_i)$`选一篇文档`$d_i$`
2. 从狄利克雷分布（即`Dirichlet`分布）`α`中取样生成文档`$d_i$`的主题分布`$θ_i$`，换言之，主题分布`$θ_i$`由超参数为`α`的`Dirichlet`分布生成
3. 从主题的多项式分布`$θ_i$`中取样生成文档`$d_i$`第`j`个词的主题`$z_{i,j}$`
4. 从狄利克雷分布（即`Dirichlet`分布）`β`中取样生成主题`$z_{i,j}$`对应的词语分布`$\phi_{z_{i, j}}$`，换言之，词语分布`$\phi_{z_{i, j}}$`由参数为`β`的`Dirichlet`分布生成
5. 从词语的多项式分布`$\phi_{z_{i, j}}$`中采样最终生成词语`$w_{i,j}$`

### 5. `pLSA` 模型 VS `LDA` 模型

pLSA跟LDA的本质区别就在于它们去估计未知参数所采用的思想不同，前者用的是==频率派思想==，后者用的是==贝叶斯派思想==。

- **频率派**把需要推断的参数`θ`看做是固定的未知常数，即概率`θ`虽然是未知的，但最起码是确定的一个值，同时，样本`X`是随机的，所以频率派重点研究==样本空间==，大部分的概率计算都是针对==样本`X`的分布==。
- **贝叶斯派**的观点则截然相反，他们认为待估计的==参数`θ`是随机变量，服从一定的分布==，而样本`X`是固定的，由于样本是固定的，所以他们重点研究的是==参数`θ`的分布==。

> ==贝叶斯派思想==的固定模式：

```math
先验分布\pi(\theta) + 样本信息\chi \Rightarrow 后验分布{\pi}(\theta | x)
```
**(一)文档生成过程**

- **pLSA**中，==主题分布和词分布确定==后，以一定的概率（`$P(z_k|d_i)$`和`$P(w_j|z_k)$`）分别选取具体的主题和词项，生成好文档。而后根据生成好的文档反推其主题分布、词分布。
- **LDA**中，不再认为主题分布（各个主题在文档中出现的概率分布）和词分布（各个词语在某个主题下出现的概率分布）是唯一确定的（而是随机变量），而是有很多种可能。`LDA`为它们弄了两个`Dirichlet`先验参数，==这个`Dirichlet`先验为某篇文档随机抽取出某个主题分布和词分布==。

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/b68e61b9fa9321cee59f807071b5bfc5/53949E612CEF40F4BF64A25C4FFC5512?ynotemdtimestamp=1564581225297" />
    <strong>Fig</strong>. pLSA VS. LDA
</p>

**(二)参数估计**

- 在**pLSA**中，我们使用==EM算法去估计“主题-词项”矩阵`ϕ`（由`$P(w_j|z_k)$`转换得到）和“文档-主题”矩阵`Θ`（由`$P(z_k|d_i)$`转换得到）这两个参数==，而且这两参数都是个固定的值，只是未知，使用的思想其实就是==极大似然估计MLE==。
- 在**LDA**中，估计`ϕ`、`Θ`这两未知参数可以用==变分(Variational inference)-EM算法==，也可以用==gibbs采样==，前者的思想是==最大后验估计MAP==（MAP与MLE类似，都把未知参数当作固定的值），==后者的思想是贝叶斯估计==。

[详情见博客。](https://blog.csdn.net/v_JULY_v/article/details/41209515)
