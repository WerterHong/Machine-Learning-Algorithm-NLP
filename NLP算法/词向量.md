## 词向量

- NLP-机器学习笔试面试题解析 [Github链接](https://github.com/WerterHong/Machine-Learning-Algorithm-NLP/)
- **词向量** (若公式显示错误，请点击此链接) [有道云笔记](http://note.youdao.com/noteshare?id=40ce20b8d71773d6830bb3c464be4d37&sub=D232E45B604E4B1A9EF93206ADCDD636)

文本表示方法大致分为：

1. 基于`one-hot`、`tf-idf`、`textrank`等的`bag-of-words`；
2. 主题模型：`LSA（SVD）`、`pLSA`、`LDA`；
3. 基于词向量的固定表征：`word2vec`、`fastText`、`glove`
4. 基于词向量的动态表征：`elmo`、`GPT`、`bert`

> 各种词向量的特点：
>
> (1) `One-hot`表示：维度灾难、语义鸿沟；
>
> (2) 分布式表示 (`distributed representation`，==相同上下文语境的词有相似含义==) ：
>
> - 矩阵分解（`LSA`）：利用全局语料特征，但`SVD`求解==计算复杂度大==；
> - 基于`NNLM/RNNLM`的词向量：词向量为==副产物==，存在==效率不高==等问题；
> - `word2vec`、`fastText`：==优化效率高==，但是基于==局部语料==；
> - `glove`：基于==全局预料==，并结合==上下文语境==构建词向量，结合了`LSA`和`word2vec`的优点；
>
> 上述产生词向量是==固定表征==的，无法解决==一词多义==等问题(美国总统**川普**)。引入基于语言模型的动态表征方法:
> - `elmo`、`GPT`、`bert`：==动态特征==；

### 1. `Word2vec`

详情见[`word2vec`](http://note.youdao.com/noteshare?id=d3385e2e4599150c59a78cfd34abdfb6&sub=7516C8DB76FE4331B749885749BFE467)。

### 2. `FastText`

`FastText`提供简单而高效的文本分类和表征学习的方法。典型应用场景:==带监督的文本分类问题==。

`FastText`方法包含三部分，**模型架构，层次`SoftMax`和`N-gram`子词特征**。

#### 2.1 模型架构

`FastText`是`word2vec`所衍生出来的，`fastText`的架构和`word2vec`中的[`CBOW`](http://note.youdao.com/noteshare?id=d3385e2e4599150c59a78cfd34abdfb6&sub=7516C8DB76FE4331B749885749BFE467)的架构类似。

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/40ce20b8d71773d6830bb3c464be4d37/1C9DF441A03145EDB69A1BE05740B209?ynotemdtimestamp=1565426410616" width=500 />
    <br/>
    <strong>Fig</strong>. FastText 模型架构
</p>

其中`$x_1,x_2,...,x_{N−1},x_N$`表示一个文本中的`n-gram`向量，每个特征是词向量的平均值。

`CBOW`模型中，隐藏层是简单的求和。而`Fasttext`模型架构中，隐藏层由输入层求合并平均，乘以权重矩阵`A`得到的。相当于==各个词向量加权求和==，作为该句子的`vector`。输出层是==由隐藏层乘以权重矩阵`B`得到==的。

#### 2.2 层次 `Softmax`

层次`softmax`实质上是将一个==全局多分类==的问题，转化成为了==若干个二元分类问题==，从而将计算复杂度从`O(V)`降到`O(logV)`。每个二元分类问题，由一个==基本的逻辑回归单元==来实现。

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/40ce20b8d71773d6830bb3c464be4d37/C9DA8122C63D48088996D3501E348D5D?ynotemdtimestamp=1565426410616" />
    <br/>
    <strong>Fig</strong>. 层次 softmax 示例
</p>

> `Huffman`树构造过程：
>
> 输入：`n`个节点及其对应权值`$w_1,w_2,...,w_n$`
>
> 输出：对应的`Huffman`树
>
> 1. 将`$w_1,w_2,...,w_n$`看成由`n`棵仅有一个节点的树组成的森林
> 2. 在森林中选择权值最小的两棵树进行合并，得到一棵新树.新树以原来两棵子树作为左右子树，井且新树根结点的权值等于左右子树的权值之和
> 3. 用新树替换原来森林中权值最小的那两棵树
> 4. 重复步骤2和3，直到森林中仅有一棵树的为止

详情见`word2vec`中[层次`softmax`](http://note.youdao.com/noteshare?id=d3385e2e4599150c59a78cfd34abdfb6&sub=7516C8DB76FE4331B749885749BFE467)简介。

#### 2.3 `n-gram` 特征

为了将==词序==`word order`考虑进来，`fastext`使用了**n-gram 特征**。

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/40ce20b8d71773d6830bb3c464be4d37/CD5931AC130C4240ADD68C59344214E6?ynotemdtimestamp=1565426410616" />
    <br/>
    <strong>Fig</strong>. n-gram 特征
</p>

`Fasttext`采用了`Hash`桶的方式，把所有的`n-gram`都哈希到`buckets`个桶中，哈希到同一个桶的所有`n-gram`共享一个`embedding vector`。

- 使用`n-gram`有如下优点：

1. 为==罕见的单词生成更好的单词向量==：根据上面的字符级别的`n-gram`来说，即是这个单词出现的次数很少，但是组成单词的字符和其他单词有共享的部分，因此这一点可以优化生成的单词向量
2. 比较好的==处理`OOV`（`out-of-vocabulary`）词==：在词汇单词中，即使单词没有出现在训练语料库中，仍然可以从字符级`n-gram`中构造单词的词向量
3. `n-gram`可以让模型学习到==局部单词顺序的部分信息==，如果不考虑`n-gram`则便是取每个单词，这样无法考虑到词序所包含的信息，即也可理解为上下文信息，因此通过`n-gram`的方式关联相邻的几个词，这样会让模型在训练的时候保持词序信息

> `CBow/Skip-gram` 缺点：
> 1. `CBow/Skip-gram`是一个`local context window`的方法，比如使用`NS`来训练，==缺乏了整体的词和词的关系==，负样本采用`sample`的方式会==缺失词的关系信息==
> 2. 直接训练`Skip-Gram`类型的算法，很容易使得==高曝光词汇得到过多的权重==
> 3. ==多义词处理乏力==，因为使用了唯一词向量
>
> `Global Vector(GloVe)`融合了矩阵分解`Latent Semantic Analysis (LSA)`的全局统计信息和`local context window`优势。融入==全局的先验统计信息==，可以加快模型的==训练速度==，又可以控制==词的相对权重==。


### 3. `GloVe`[论文](https://www.aclweb.org/anthology/D14-1162)

> `GloVe`的全称叫`Global Vectors for Word Representation`，它是一个基于==全局词频统计==（`count-based & overall statistics`）的词表征（`word representation`）工具。

#### 3.1 `GloVe`实现步骤

```
graph LR
开始-->统计共现矩阵
统计共现矩阵-->训练词向量
训练词向量-->结束
```

- 根据语料库（`corpus`）==构建一个共现矩阵==`$X$`（`Co-ocurrence Matrix`）。矩阵中的每一个元素`$X_{i,j}$`代表单词`i`和上下文单词`j`在特定大小的上下文窗口（`context window`）内共同出现的次数。
> 根据两个单词在上下文窗口的距离`d`，提出了一个==衰减函数==（`decreasing weighting`）：`decay = 1/d`用于计算权重，也就是说距离越远的两个单词所占总计数（`total count`）的权重越小。
- 构建词向量（`Word Vector`）和共现矩阵（`Co-ocurrence Matrix`）之间的近似关系

```math
w_{i}^{T} \tilde{w}_{k}+b_{i}+\tilde{b}_{k}=\log \left(X_{i k}\right)
```
> `$w_{i}^{T}$`和`$\tilde{w}_{k}$`是==最终要求解的词向量==（the sum `$W+\tilde{W}$` as our word vectors），`b`是`bias`项。

- 构造损失函数（`loss function`：`mean square loss`）

```math
J = \sum_{i,j=1}^{V} f(X_{ij})(w_{i}^{T}\tilde{w_{j}} + b_i + \tilde{b_j} – \log(X_{ij}) )^2 \tag{2}
```
> `V`是词表大小，`$f(X_{ij})$`是权重函数，满足：
> 1. `f(0) = 0`. If `f` is viewed as a continuous function, it should vanish as `x → 0` fast enough that the `$\lim_{x→0} f(x) \log ^2 x$` is finite. (不共现权重为`0`)
> 2. `f(x)` should be non-decreasing so that rare co-occurrences are not overweighted. (到达一定程度之后应该不再增加)
> 3. `f(x)` should be relatively small for large values of `x`, so that frequent co-occurrences are not overweighted. (非递减函数)

论文采用如下分段函数:

```math
f(x)=\left\{\begin{array}{cc}{\left(x / x_{\max }\right)^{\alpha}} & {\text { if } x<x_{\max }} \\ {1} & {\text { otherwise }}\end{array}\right.
```
> 论文中的所有实验，`α`的取值都是`0.75`，而`$x_{max}$`取值都是100。

`α = 3/4` 时的权重函数如下所示：

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/40ce20b8d71773d6830bb3c464be4d37/A3A6AD062BC84FF49AD1A3A3D15C2413?ynotemdtimestamp=1565426410616" width=500 />
    <br/>
    <strong>Fig</strong>. α = 3/4 时得权重函数
</p>

#### 3.2 推导过程

> 定义：

```math
X_{i}=\sum_{j=1}^{N} X_{i, j}

P_{i, k}=\frac{X_{i, k}}{X_{i}}

ratio_{i,j,k}=\frac{P_{i, k}}{P_{j, k}}
```
> `$P_{i,k}$`条件概率，表示单词`k`出现在单词`i`语境中的概率；`$ratio_{i,j,k}$`表示两个条件概率的比率，规律如下：

| `$ratio_{i,j,k}$`的值 | 单词`j,k`相关 | 单词`j,k`不相关 |
|:--------------:|:-----------:|:-------------:|
|   单词`i,k`相关  |    趋近`1`   |      很大     |
|  单词`i,k`不相关 |     很小    |     趋近`1`     |

> 用词向量`$v_i,v_j,v_k$`通过某种函数`$g(v_i,v_j,v_k)$`计算`$ratio_{i,j,k}$`，使得==词向量与共现矩阵具有很好的一致性==，也就说明词向量中蕴含了共现矩阵中所蕴含的信息。即：

```math
\frac{P_{i, k}}{P_{j, k}}=r a t i o_{i, j, k}=g\left(v_{i}, v_{j}, v_{k}\right)
```
> 用二者的差方来作为代价函数（复杂度太高`O(N*N*N)`）：

```math
J=\sum_{i, j, k}^{N}\left(\frac{P_{i, k}}{P_{j, k}}-g\left(v_{i}, v_{j}, v_{k}\right)\right)^{2}
```
> 考虑三个方面：
> 1. 在线性空间中考察两个向量的相似性，不失**线性**地考察：`$v_i-v_j$`
> 2. `$ratio_{i,j,k}$`是个标量，即`$g(v_i,v_j,v_k)$`也应该是个标量：**內积** `$(v_i-v_j) ^Tv_k$`
> 3. 差形式变成**商形式**，进而等式两边分子分母对应相等，套一层指数运算`exp()`：`$g(v_{i}, v_{j}, v_{k})=\exp ((v_{i}-v_{j})^{T} v_{k})$`

> 简化得：

```math
\frac{P_{i, k}}{P_{j, k}}=\frac{\exp \left(v_{i}^{T} v_{k}\right)}{\exp \left(v_{j}^{T} v_{k}\right)}
```
> 让上式分子对应相等，分母对应相等，即：

```math
P_{i, k}=\exp \left(v_{i}^{T} v_{k}\right) , \quad  P_{j, k}=\exp \left(v_{j}^{T} v_{k}\right)
```
> 取对数得：

```math
\log \left(P_{i, j}\right)=v_{i}^{T} v_{j}
```
> 代价函数简化为：

```math
J=\sum^N_{i,j}(\log (P_{i,j})-v_{i}^{T} v_{j})
```
> 但是取对数后对称性出现问题：

```math
\log (P_{i,j}) \neq \log (P_{j,i}), \quad v_{i}^{T} v_{j}=v_{j}^{T} v_{i}
```
> 代价函数中的条件概率展开：

```math
\log \left(P_{i, j}\right)=\log \left(\frac{X_{i,j}}{X_i}\right)=\log (X_{i,j})-\log (X_i)=v_{i}^{T} v_{j}
```
> 变换后为（添了一个偏差项`$b_j$`，并将`$\log (X_{i})$`吸收到偏差项`$b_i$` 中）：

```math
\log (X_{i,j})=v_{i}^{T} v_{j}+b_i+b_j
```
> 转换为最小二乘得最优化问题（`log-bilinear`模型）：

```math
J = \sum_{i,j=1}^{V}(w_{i}^{T}\tilde{w_{j}} + b_i + \tilde{b_j} – \log(X_{ij}) )^2 \tag{2}
```

### 4. `ELMo`[论文](https://arxiv.org/pdf/1802.05365.pdf)

> `ELMo`的全称是`Embeddings from Language Models`，是一种==新型深度语境化词表征==。
>
>优势：`ELMo`利用预训练好的双向语言模型，可对词进行==复杂特征==(如==句法和语义==)和词在语言==上下文语境==中的变化进行建模(==词汇多义性==)。

#### 4.1 `Bidirectional language models`

给定`N`个`tokens`的序列`$(t_1,t_2,...,t_N)$`，前向`language model`通过前`k-1`个输入序列`$(t_1,t_2,...,t_k)$`的`hidden`表示，预测第`k`个位置的`token`。

```math
p\left(t_{1}, t_{2}, \ldots, t_{N}\right)=\prod_{k=1}^{N} p\left(t_{k} | t_{1}, t_{2}, \ldots, t_{k-1}\right)
```
反向`language model`就是给定后面的序列`$(t_{k+1},t_{k+2},...,t_N)$`，预测之前第`k`个位置的`token`。

```math
p\left(t_{1}, t_{2}, \ldots, t_{N}\right)=\prod_{k=1}^{N} p\left(t_{k} | t_{k+1}, t_{k+2}, \ldots, t_{N}\right)
```

`biLM`训练过程的目标就是最大化：

```math
\sum_{k=1}^{N}\left(\log p\left(t_{k} | t_{1}, \ldots, t_{k-1} ; \Theta_{x}, \vec{\Theta}_{L S T M}, \Theta_{s}\right)+\log p\left(t_{k} | t_{k+1}, \ldots, t_{N} ; \Theta_{x}, \overleftarrow \Theta_{L S T M}, \Theta_{s}\right) \right)
```

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/40ce20b8d71773d6830bb3c464be4d37/E9077F6084EE4CA4ABE285DB1FF8F27E?ynotemdtimestamp=1565443615603" width=600 />
    <br/>
    <strong>Fig</strong>. 序列标注任务中的 bi-LM（右边）
</p>

#### 4.2 `ELMo` 算法

`ELMo`对于每个`$token:t_{k}$` , 通过一个`L`层的`biLM`计算出`2L+1`个表示:

```math
\begin{aligned} R_{k} &=\left\{\mathbf{x}_{k}^{L M}, \overrightarrow{\mathbf{h}}_{k, j}^{L M}, \overleftarrow \mathbf{h}_{k, j}^{L M} | j=1, \ldots, L\right\} =\left\{\mathbf{h}_{k, j}^{L M} | j=0, \ldots, L\right\} \end{aligned}
```
其中`$\mathbf{h}_{k, 0}^{L M}$`是对`token`进行直接编码的结果(这里是字符通过`CNN`编码)，`$\mathbf{h}_{k, j}^{L M}=[\overrightarrow{\mathbf{h}}_{k, j}^{L M}, \overleftarrow \mathbf{h}_{k, j}^{L M}]$`是每个`biLSTM`层输出的结果。==不同层的`biLM`的输出的`token`表示对于不同的任务效果不同==。

应用中将`ELMo`中所有层的输出`$R_{k}$`压缩为单个向量，`$\mathbf{E L M o}_{k}=E\left(R_{k};\Theta\right)$`，最简单的压缩方法是取最上层的结果做为`token`的表示：`$E(R_{k})=h^{LM}_{k,L}$`，更通用的做法是通过一些参数来==联合所有层==的信息:

```math
\mathbf{E L M o}_{k}^{t a s k}=E\left(R_{k} ; \Theta^{t a s k}\right)=\gamma^{t a s k} \sum_{j=0}^{L} s_{j}^{t a s k} \mathbf{h}_{k, j}^{L M}
```
其中`$s^{task}$`是`softmax`规范化的权重(`softmax-normalized weights`)，`$\gamma^{task}$`是缩放因子，允许任务模型`scale`整体`ELMo`向量(`allows the task model to scale the entire ELMo vector`，论文是`1024`维)。

**使用`ELMo`步骤**：

1. 产生`pre-trained biLM`模型。模型由两层`bi-LSTM`组成，之间用`residual connection`连接起来
2. 在任务语料上`fine tuning`上一步得到的`biLM`模型。可以把这一步看为`biLM`的`domain transfer`
3. 利用`ELMo`的`word embedding`来对任务进行训练。

> `ELMo`词向量的两种使用方法：
> 1. 直接将`ELMo`词向量`$ELMo_k$`与普通的词向量`$x_k$`拼接（`concat`）`$[x_k;ELMo_k]$`
> 2. 直接将`ELMo`词向量`$ELMo_k$`与隐层输出向量`$h_k$`拼接（`concat`）`$[h_k;ELMo_k]$`

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/40ce20b8d71773d6830bb3c464be4d37/03E7DBF2FDCF4A26BFF0CFD792024028?ynotemdtimestamp=1565443615603" width=600 />
    <br/>
    <strong>Fig</strong>. ELMo 模型与应用
</p>

> `ELMo`存在的弱点：
> 1. `ELMo`使用`LSTM`==抽取特征能力远弱于`Transformer`==
> 2. `ELMo`采用==双向拼接方式融合特征能力偏弱==（相较于`bert`一体化的融合特征方式）

### 5. `OpenAI GPT`[论文](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)

`GPT(Generative Pre-Training)`是一种基于`fine-tuning`的模型。`GPT`采用两阶段过程，第一个阶段是利用语言模型(`transformer`)进行预训练，第二阶段通过`fine-tuning`的模式解决下游任务。

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/40ce20b8d71773d6830bb3c464be4d37/73BFED013538455598CF075BF5238F01?ynotemdtimestamp=1565443615603" width=650 />
    <br/>
    <strong>Fig</strong>. (左) Transformer 结构， (右) Fine-tuning 结构
</p>

#### 5.1 无监督`pre-training`

处理非监督文本`$(x_1,x_2,...,x_m)$`的方法是用语言模型去最大化语言模型的极大似然：

```math
L_{1}(X)=\sum_{i} \log P\left(x_{i} | x_{i-k}, \ldots, x_{i-1} : \theta\right)
```
`GPT`使用多层`Transformer`的`decoder`的语言模型，应用`multi-headed self-attention`在处理==输入的文本加上位置信息的前馈网络==，输出是==词的概率分布==。

```math
 h_{0} =U W_{e}+W_{p}, \quad U=(u_{-k},...,u_{-1})

 h_{l} =\operatorname{transformer\_block}\left(h_{l-1}\right), \quad \forall i \in[1, n]

 P(u) =\operatorname{softmax}\left(h_{n} W_{e}^{T}\right)
```
其中`U`是`tokens`的上下文向量，`n`是层数，`$W_e$`是`token embedding`矩阵，`$W_p$`是`position embedding`矩阵。

#### 5.2 有监督`fine-tuning`

假设标签数据集`C`，对于每个实例，有`$tokens:\{x^1,x^2,...,x^m\}$`以及其标签`y`。输入`$\{x^1,x^2,...,x^m\}$`，经过我们预训练的模型获得输出向量`$h^m_l$`，然后经过线性层`$W_{y}$`和`softmax`来预测标签:

```math
P\left(y | x^{1}, \ldots, x^{m}\right)=\operatorname{softmax}\left(h_{l}^{m} W_{y}\right)
```
最大化如下目标函数;

```math
L_{2}({C})=\sum_{(x, y)} \log P\left(y | x^{1}, \ldots, x^{m}\right)
```
利用语言模型来辅助`fine-tuning`可以增强监督模型效果。最终的目标函数为：

```math
L_{3}({C})=L_{2}({C})+\lambda * L_{1}({C})
```

