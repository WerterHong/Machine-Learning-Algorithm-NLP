## 词向量

- NLP-机器学习笔试面试题解析 [Github链接](https://github.com/WerterHong/Machine-Learning-Algorithm-NLP/)
- **词向量** (若公式显示错误，请点击此链接) [有道云笔记](http://note.youdao.com/noteshare?id=cae824947c6f2df10736e2d1fa7b51b8&sub=FD379A7B572F4CC99C848087F3E90DD4)

文本表示方法大致分为：

1. 基于`one-hot`、`tf-idf`、`textrank`等的`bag-of-words`；
2. 主题模型：`LSA（SVD）`、`pLSA`、`LDA`；
3. 基于词向量的固定表征：`word2vec`、`fastText`、`glove`
4. 基于词向量的动态表征：`elmo`、`GPT`、`bert`

> 各种词向量的特点：
>
> (1) `One-hot`表示：维度灾难、语义鸿沟；
>
> (2) 分布式表示 (`distributed representation`，==相同上下文语境的词有相似含义==) ：
>
> - 矩阵分解（`LSA`）：利用全局语料特征，但`SVD`求解==计算复杂度大==；
> - 基于`NNLM/RNNLM`的词向量：词向量为==副产物==，存在==效率不高==等问题；
> - `word2vec`、`fastText`：==优化效率高==，但是基于==局部语料==；
> - `glove`：基于==全局预料==，并结合==上下文语境==构建词向量，结合了`LSA`和`word2vec`的优点；
>
> 上述产生词向量是==固定表征==的，无法解决==一词多义==等问题(美国总统**川普**)。引入基于语言模型的动态表征方法:
> - `elmo`、`GPT`、`bert`：==动态特征==；

### 1. `Word2vec`

详情见[`word2vec`](http://note.youdao.com/noteshare?id=2c44c1e114cab8b75824ad17073dfb43&sub=204349CD90724BC5B250A727857B11CF)。

> Why 层序 Softmax 和负采样？
>
> Ans：在每个样本中每个词的训练过程都**要遍历整个词汇表**，也就是都需要经过`softmax`归一化，**计算误差向量和梯度以更新两个词向量矩阵**（初始化不一样），当语料库规模变大、词汇表增长时，训练变得不切实际。

### 2. `FastText`

`FastText`提供简单而高效的文本分类和表征学习的方法。典型应用场景:==带监督的文本分类问题==。

`FastText`方法包含三部分，**模型架构，层次`SoftMax`和`N-gram`子词特征**。

#### 2.1 模型架构

`FastText`是`word2vec`所衍生出来的，`fastText`的架构和`word2vec`中的[`CBOW`](http://note.youdao.com/noteshare?id=2c44c1e114cab8b75824ad17073dfb43&sub=204349CD90724BC5B250A727857B11CF)的架构类似。

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/40ce20b8d71773d6830bb3c464be4d37/1C9DF441A03145EDB69A1BE05740B209?ynotemdtimestamp=1565426410616" width=500 />
    <br/>
    <strong>Fig</strong>. FastText 模型架构
</p>

其中`$x_1,x_2,...,x_{N−1},x_N$`表示一个文本中的`n-gram`向量，每个特征是词向量的平均值。

`CBOW`模型中，隐藏层是简单的求和。而`Fasttext`模型架构中，隐藏层由输入层求合并平均，乘以权重矩阵`A`得到的。相当于==各个词向量加权求和==，作为该句子的`vector`。输出层是==由隐藏层乘以权重矩阵`B`得到==的。

#### 2.2 层次 `Softmax`

层次`softmax`实质上是将一个==全局多分类==的问题，转化成为了==若干个二元分类问题==，从而将计算复杂度从`O(V)`降到`O(logV)`。每个二元分类问题，由一个==基本的逻辑回归单元==来实现。

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/40ce20b8d71773d6830bb3c464be4d37/C9DA8122C63D48088996D3501E348D5D?ynotemdtimestamp=1565426410616" />
    <br/>
    <strong>Fig</strong>. 层次 softmax 示例
</p>

> `Huffman`树构造过程：
>
> 输入：`n`个节点及其对应权值`$w_1,w_2,...,w_n$`
>
> 输出：对应的`Huffman`树
>
> 1. 将`$w_1,w_2,...,w_n$`看成由`n`棵仅有一个节点的树组成的森林
> 2. 在森林中选择权值最小的两棵树进行合并，得到一棵新树.新树以原来两棵子树作为左右子树，井且新树根结点的权值等于左右子树的权值之和
> 3. 用新树替换原来森林中权值最小的那两棵树
> 4. 重复步骤2和3，直到森林中仅有一棵树的为止

详情见`word2vec`中[层次`softmax`](http://note.youdao.com/noteshare?id=2c44c1e114cab8b75824ad17073dfb43&sub=204349CD90724BC5B250A727857B11CF)简介。

#### 2.3 `n-gram` 特征

为了将==词序==`word order`考虑进来，`fastext`使用了**n-gram 特征**。

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/40ce20b8d71773d6830bb3c464be4d37/CD5931AC130C4240ADD68C59344214E6?ynotemdtimestamp=1565426410616" />
    <br/>
    <strong>Fig</strong>. n-gram 特征
</p>

`Fasttext`采用了`Hash`桶的方式，把所有的`n-gram`都哈希到`buckets`个桶中，哈希到同一个桶的所有`n-gram`共享一个`embedding vector`。

- 使用`n-gram`有如下优点：

1. 为==罕见的单词生成更好的单词向量==：根据上面的字符级别的`n-gram`来说，即是这个单词出现的次数很少，但是组成单词的字符和其他单词有共享的部分，因此这一点可以优化生成的单词向量
2. 比较好的==处理`OOV`（`out-of-vocabulary`）词==：在词汇单词中，即使单词没有出现在训练语料库中，仍然可以从字符级`n-gram`中构造单词的词向量
3. `n-gram`可以让模型学习到==局部单词顺序的部分信息==，如果不考虑`n-gram`则便是取每个单词，这样无法考虑到词序所包含的信息，即也可理解为上下文信息，因此通过`n-gram`的方式关联相邻的几个词，这样会让模型在训练的时候保持词序信息

> `CBow/Skip-gram` 缺点：
> 1. `CBow/Skip-gram`是一个`local context window`的方法，比如使用`NS`来训练，==缺乏了整体的词和词的关系==，负样本采用`sample`的方式会==缺失词的关系信息==
> 2. 直接训练`Skip-Gram`类型的算法，很容易使得==高曝光词汇得到过多的权重==
> 3. ==多义词处理乏力==，因为使用了唯一词向量
>
> `Global Vector(GloVe)`融合了矩阵分解`Latent Semantic Analysis (LSA)`的全局统计信息和`local context window`优势。融入==全局的先验统计信息==，可以加快模型的==训练速度==，又可以控制==词的相对权重==。


### 3. `GloVe`[论文](https://www.aclweb.org/anthology/D14-1162)

> `GloVe`的全称叫`Global Vectors for Word Representation`，它是一个基于==全局词频统计==（`count-based & overall statistics`）的词表征（`word representation`）工具。

#### 3.1 `GloVe`实现步骤

```
graph LR
开始-->统计共现矩阵
统计共现矩阵-->训练词向量
训练词向量-->结束
```

- 根据语料库（`corpus`）==构建一个共现矩阵==`$X$`（`Co-ocurrence Matrix`）。矩阵中的每一个元素`$X_{i,j}$`代表单词`i`和上下文单词`j`在特定大小的上下文窗口（`context window`）内共同出现的次数。
> 根据两个单词在上下文窗口的距离`d`，提出了一个==衰减函数==（`decreasing weighting`）：`decay = 1/d`用于计算权重，也就是说距离越远的两个单词所占总计数（`total count`）的权重越小。
- 构建词向量（`Word Vector`）和共现矩阵（`Co-ocurrence Matrix`）之间的近似关系

```math
w_{i}^{T} \tilde{w}_{k}+b_{i}+\tilde{b}_{k}=\log \left(X_{i k}\right)
```
> `$w_{i}^{T}$`和`$\tilde{w}_{k}$`是==最终要求解的词向量==（the sum `$W+\tilde{W}$` as our word vectors），`b`是`bias`项。

- 构造损失函数（`loss function`：`mean square loss`）

```math
J = \sum_{i,j=1}^{V} f(X_{ij})(w_{i}^{T}\tilde{w_{j}} + b_i + \tilde{b_j} – \log(X_{ij}) )^2 \tag{2}
```
> `V`是词表大小，`$f(X_{ij})$`是权重函数，满足：
> 1. `f(0) = 0`. If `f` is viewed as a continuous function, it should vanish as `x → 0` fast enough that the `$\lim_{x→0} f(x) \log ^2 x$` is finite. (不共现权重为`0`)
> 2. `f(x)` should be non-decreasing so that rare co-occurrences are not overweighted. (到达一定程度之后应该不再增加)
> 3. `f(x)` should be relatively small for large values of `x`, so that frequent co-occurrences are not overweighted. (非递减函数)

论文采用如下分段函数:

```math
f(x)=\left\{\begin{array}{cc}{\left(x / x_{\max }\right)^{\alpha}} & {\text { if } x<x_{\max }} \\ {1} & {\text { otherwise }}\end{array}\right.
```
> 论文中的所有实验，`α`的取值都是`0.75`，而`$x_{max}$`取值都是100。

`α = 3/4` 时的权重函数如下所示：

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/40ce20b8d71773d6830bb3c464be4d37/A3A6AD062BC84FF49AD1A3A3D15C2413?ynotemdtimestamp=1565426410616" width=500 />
    <br/>
    <strong>Fig</strong>. α = 3/4 时得权重函数
</p>

#### 3.2 推导过程

> 定义：

```math
X_{i}=\sum_{j=1}^{N} X_{i, j}

P_{i, k}=\frac{X_{i, k}}{X_{i}}

ratio_{i,j,k}=\frac{P_{i, k}}{P_{j, k}}
```
> `$P_{i,k}$`条件概率，表示单词`k`出现在单词`i`语境中的概率；`$ratio_{i,j,k}$`表示两个条件概率的比率，规律如下：

| `$ratio_{i,j,k}$`的值 | 单词`j,k`相关 | 单词`j,k`不相关 |
|:--------------:|:-----------:|:-------------:|
|   单词`i,k`相关  |    趋近`1`   |      很大     |
|  单词`i,k`不相关 |     很小    |     趋近`1`     |

> 用词向量`$v_i,v_j,v_k$`通过某种函数`$g(v_i,v_j,v_k)$`计算`$ratio_{i,j,k}$`，使得==词向量与共现矩阵具有很好的一致性==，也就说明词向量中蕴含了共现矩阵中所蕴含的信息。即：

```math
\frac{P_{i, k}}{P_{j, k}}=r a t i o_{i, j, k}=g\left(v_{i}, v_{j}, v_{k}\right)
```
> 用二者的差方来作为代价函数（复杂度太高`O(N*N*N)`）：

```math
J=\sum_{i, j, k}^{N}\left(\frac{P_{i, k}}{P_{j, k}}-g\left(v_{i}, v_{j}, v_{k}\right)\right)^{2}
```
> 考虑三个方面：
> 1. 在线性空间中考察两个向量的相似性，不失**线性**地考察：`$v_i-v_j$`
> 2. `$ratio_{i,j,k}$`是个标量，即`$g(v_i,v_j,v_k)$`也应该是个标量：**內积** `$(v_i-v_j) ^Tv_k$`
> 3. 差形式变成**商形式**，进而等式两边分子分母对应相等，套一层指数运算`exp()`：`$g(v_{i}, v_{j}, v_{k})=\exp ((v_{i}-v_{j})^{T} v_{k})$`

> 简化得：

```math
\frac{P_{i, k}}{P_{j, k}}=\frac{\exp \left(v_{i}^{T} v_{k}\right)}{\exp \left(v_{j}^{T} v_{k}\right)}
```
> 让上式分子对应相等，分母对应相等，即：

```math
P_{i, k}=\exp \left(v_{i}^{T} v_{k}\right) , \quad  P_{j, k}=\exp \left(v_{j}^{T} v_{k}\right)
```
> 取对数得：

```math
\log \left(P_{i, j}\right)=v_{i}^{T} v_{j}
```
> 代价函数简化为：

```math
J=\sum^N_{i,j}(\log (P_{i,j})-v_{i}^{T} v_{j})
```
> 但是取对数后对称性出现问题：

```math
\log (P_{i,j}) \neq \log (P_{j,i}), \quad v_{i}^{T} v_{j}=v_{j}^{T} v_{i}
```
> 代价函数中的条件概率展开：

```math
\log \left(P_{i, j}\right)=\log \left(\frac{X_{i,j}}{X_i}\right)=\log (X_{i,j})-\log (X_i)=v_{i}^{T} v_{j}
```
> 变换后为（添了一个偏差项`$b_j$`，并将`$\log (X_{i})$`吸收到偏差项`$b_i$` 中）：

```math
\log (X_{i,j})=v_{i}^{T} v_{j}+b_i+b_j
```
> 转换为最小二乘得最优化问题（`log-bilinear`模型）：

```math
J = \sum_{i,j=1}^{V}(w_{i}^{T}\tilde{w_{j}} + b_i + \tilde{b_j} – \log(X_{ij}) )^2 \tag{2}
```

### 4. `ELMo`[论文](https://arxiv.org/pdf/1802.05365.pdf)

> `ELMo`的全称是`Embeddings from Language Models`，是一种==新型深度语境化词表征==。
>
>优势：`ELMo`利用预训练好的双向语言模型，可对词进行==复杂特征==(如==句法和语义==)和词在语言==上下文语境==中的变化进行建模(==词汇多义性==)。

#### 4.1 `Bidirectional language models`

给定`N`个`tokens`的序列`$(t_1,t_2,...,t_N)$`，前向`language model`通过前`k-1`个输入序列`$(t_1,t_2,...,t_k)$`的`hidden`表示，预测第`k`个位置的`token`。

```math
p\left(t_{1}, t_{2}, \ldots, t_{N}\right)=\prod_{k=1}^{N} p\left(t_{k} | t_{1}, t_{2}, \ldots, t_{k-1}\right)
```
反向`language model`就是给定后面的序列`$(t_{k+1},t_{k+2},...,t_N)$`，预测之前第`k`个位置的`token`。

```math
p\left(t_{1}, t_{2}, \ldots, t_{N}\right)=\prod_{k=1}^{N} p\left(t_{k} | t_{k+1}, t_{k+2}, \ldots, t_{N}\right)
```

`biLM`训练过程的目标就是最大化：

```math
\sum_{k=1}^{N}\left(\log p\left(t_{k} | t_{1}, \ldots, t_{k-1} ; \Theta_{x}, \vec{\Theta}_{L S T M}, \Theta_{s}\right)+\log p\left(t_{k} | t_{k+1}, \ldots, t_{N} ; \Theta_{x}, \overleftarrow \Theta_{L S T M}, \Theta_{s}\right) \right)
```

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/40ce20b8d71773d6830bb3c464be4d37/E9077F6084EE4CA4ABE285DB1FF8F27E?ynotemdtimestamp=1565443615603" width=600 />
    <br/>
    <strong>Fig</strong>. 序列标注任务中的 bi-LM（右边）
</p>

#### 4.2 `ELMo` 算法

`ELMo`对于每个`$token:t_{k}$` , 通过一个`L`层的`biLM`计算出`2L+1`个表示:

```math
\begin{aligned} R_{k} &=\left\{\mathbf{x}_{k}^{L M}, \overrightarrow{\mathbf{h}}_{k, j}^{L M}, \overleftarrow \mathbf{h}_{k, j}^{L M} | j=1, \ldots, L\right\} =\left\{\mathbf{h}_{k, j}^{L M} | j=0, \ldots, L\right\} \end{aligned}
```
其中`$\mathbf{h}_{k, 0}^{L M}$`是对`token`进行直接编码的结果(这里是字符通过`CNN`编码)，`$\mathbf{h}_{k, j}^{L M}=[\overrightarrow{\mathbf{h}}_{k, j}^{L M}, \overleftarrow \mathbf{h}_{k, j}^{L M}]$`是每个`biLSTM`层输出的结果。==不同层的`biLM`的输出的`token`表示对于不同的任务效果不同==。

应用中将`ELMo`中所有层的输出`$R_{k}$`压缩为单个向量，`$\mathbf{E L M o}_{k}=E\left(R_{k};\Theta\right)$`，最简单的压缩方法是取最上层的结果做为`token`的表示：`$E(R_{k})=h^{LM}_{k,L}$`，更通用的做法是通过一些参数来==联合所有层==的信息:

```math
\mathbf{E L M o}_{k}^{t a s k}=E\left(R_{k} ; \Theta^{t a s k}\right)=\gamma^{t a s k} \sum_{j=0}^{L} s_{j}^{t a s k} \mathbf{h}_{k, j}^{L M}
```
其中`$s^{task}$`是`softmax`规范化的权重(`softmax-normalized weights`)，`$\gamma^{task}$`是缩放因子，允许任务模型`scale`整体`ELMo`向量(`allows the task model to scale the entire ELMo vector`，论文是`1024`维)。

**使用`ELMo`步骤**：

1. 产生`pre-trained biLM`模型。模型由两层`bi-LSTM`组成，之间用`residual connection`连接起来
2. 在任务语料上`fine tuning`上一步得到的`biLM`模型。可以把这一步看为`biLM`的`domain transfer`
3. 利用`ELMo`的`word embedding`来对任务进行训练。

> `ELMo`词向量的两种使用方法：
> 1. 直接将`ELMo`词向量`$ELMo_k$`与普通的词向量`$x_k$`拼接（`concat`）`$[x_k;ELMo_k]$`
> 2. 直接将`ELMo`词向量`$ELMo_k$`与隐层输出向量`$h_k$`拼接（`concat`）`$[h_k;ELMo_k]$`

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/40ce20b8d71773d6830bb3c464be4d37/03E7DBF2FDCF4A26BFF0CFD792024028?ynotemdtimestamp=1565443615603" width=600 />
    <br/>
    <strong>Fig</strong>. ELMo 模型与应用
</p>

> `ELMo`存在的弱点：
> 1. `ELMo`使用`LSTM`==抽取特征能力远弱于`Transformer`==
> 2. `ELMo`采用==双向拼接方式融合特征能力偏弱==（相较于`bert`一体化的融合特征方式）

### 5. `OpenAI GPT`[论文](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)

> Two challenges：
>
> **First**, it is unclear what type of ==optimization objectives are most effective== at learning text representations that are useful for transfer
>
> **Second**, there is no consensus on the most effective way to ==transfer these learned representations to the target task==

`GPT(Generative Pre-Training)`是一种基于`fine-tuning`的模型。`GPT`采用两阶段过程，第一个阶段是利用语言模型(`transformer`)进行预训练，第二阶段通过`fine-tuning`的模式解决下游任务。

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/40ce20b8d71773d6830bb3c464be4d37/73BFED013538455598CF075BF5238F01?ynotemdtimestamp=1565443615603" width=650 />
    <br/>
    <strong>Fig</strong>. (左) Transformer 结构， (右) Fine-tuning 结构
</p>

#### 5.1 无监督`pre-training`

处理非监督文本`$(x_1,x_2,...,x_m)$`的方法是用语言模型去最大化语言模型的极大似然：

```math
L_{1}(X)=\sum_{i} \log P\left(x_{i} | x_{i-k}, \ldots, x_{i-1} : \theta\right)
```
`GPT`使用多层`Transformer`的`decoder`的语言模型，应用`multi-headed self-attention`在处理==输入的文本加上位置信息的前馈网络==，输出是==词的概率分布==。

```math
 h_{0} =U W_{e}+W_{p}, \quad U=(u_{-k},...,u_{-1})

 h_{l} =\operatorname{transformer\_block}\left(h_{l-1}\right), \quad \forall i \in[1, n]

 P(u) =\operatorname{softmax}\left(h_{n} W_{e}^{T}\right)
```
其中`U`是`tokens`的上下文向量，`n`是层数，`$W_e$`是`token embedding`矩阵，`$W_p$`是`position embedding`矩阵。

#### 5.2 有监督`fine-tuning`

假设标签数据集`C`，对于每个实例，有`$tokens:\{x^1,x^2,...,x^m\}$`以及其标签`y`。输入`$\{x^1,x^2,...,x^m\}$`，经过我们预训练的模型获得输出向量`$h^m_l$`，然后经过线性层`$W_{y}$`和`softmax`来预测标签:

```math
P\left(y | x^{1}, \ldots, x^{m}\right)=\operatorname{softmax}\left(h_{l}^{m} W_{y}\right)
```
最大化如下目标函数;

```math
L_{2}({C})=\sum_{(x, y)} \log P\left(y | x^{1}, \ldots, x^{m}\right)
```
利用语言模型来辅助`fine-tuning`可以增强监督模型效果。最终的目标函数为：

```math
L_{3}({C})=L_{2}({C})+\lambda * L_{1}({C})
```

> `GPT 1.0`采取预训练+`FineTuning`两个阶段，它采取`Transformer`作为特征抽取器。预训练阶段采用“==单向语言模型==”作为训练任务，把语言知识编码到`Transformer`里。第二阶段，在第一阶段训练好的模型基础上，通过`Finetuning`来做具体的NLP任务。
>
> `GPT 2.0`把`GPT 1.0`第二阶段的`Finetuning`做有监督地下游`NLP`任务，换成了==无监督地做下游任务==。(`Transformer Big` 24层 -> 48层)。`GPT 2.0`准备用==更多的训练数据==来做预训练，更大的模型，更多的参数，意味着更高的模型容量，容纳更多的`NLP`知识。比使用专有数据集来的==通用性更强==，更能理解语言和知识逻辑，可以用于任意领域的下游任务。过滤出==高质量数据==。

> - 为什么GPT 2.0第二阶段不通过Finetuning去有监督地做下游任务呢？
>
> Ans：`Transformer`采用更复杂的模型能学到更多更好的NLP通用知识，说明通用性好；而这无疑，如果第二阶段仍然采取`Finetuning`，对下游任务的提升效果是可以很乐观地期待的。
> - 在预训练阶段，为什么`GPT 2.0`仍使用单向语言模型，而不是双向语言模型呢？
>
> Ans：`vs. Bert`
> - `GPT 2.0`怎么做文本摘要任务？
>
> Ans：按照时间顺序拼接出输出内容作为翻译结果或者摘要结果。

### 6. `BERT`[论文](https://arxiv.org/pdf/1810.04805.pdf)

> `ELMo` 和 `OpenAI GPT`均采用了一样的目标函数(==语言模型的极大似然==)以及==单向的语言模型==，没有很好的利用==上下文==的信息。
>
> `ELMo`利用了正向和反向的语言模型，可本质上仍然是两个`unidirectional`模型的==拼接==（`concat`）。

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/40ce20b8d71773d6830bb3c464be4d37/5D43F8D60BBB4C129D4C46EFF101142E?ynotemdtimestamp=1565595544561" />
    <br/>
    <strong>Fig</strong>. BERT vs. OpenAI GPT vs. ELMo
</p>

`BERT(Bidirectional Encoder Representations from Transformers)`相对于`ELMo`和`OpenAI GPT`，主要工作点有：
1. 采取新的预训练的==目标函数==：`masked language model`，主要思想是随机`mask`输入中的一些`tokens`然后在预训练中对它们进行训练。可以==学习到表征能够融合两个方向上的`context`==。
2. 增加==句子级别的任务==：`next sentence prediction`，具体做法是随即替换一些句子，然后利用上一句进行`IsNext/NotNext`的预测。可以理解==句子之间的关系==。

#### 6.1 模型架构

`BERT`有两种模型：

```math
BERT_{BASE}: (L=12, H=768, A=12, Total Parameters=110M)

BERT_{LARGE}: (L=24, H=1024, A=16, Total Parameters=340M)
```
其中`L`是`Tansformer blocks`的层数，`H`是隐层的大小，`A`是自注意力头的数量。`In all cases we set the feed-forward/filter size to be 4H, i.e., 3072 for the H = 768 and 4096 for the H = 1024.`。

> `$BERT_{BASE}$`与`OpenAI GPT`模型大小一样，但是`BERT Transformer`使用双向的自注意力，而`OpenAI GPT Tranformer`只使用上文约束的自注意力。

#### 6.2 输入表示

`BERT`输入表示即可以是一个句子也可以一对句子(`e.g.`, <问题，答案>)，对于每一个`token`, 它的表征由==词表征==`token embedding`, ==段表征==`(segment embedding)`和==位置表征==`(position embedding)`相加产生。

- `WordPiece embeddings: 30,000 token vocabulary`
- `position embedding`: `512`位，即句子的最大长度
- 每句话的第一个`token`总是`[CLS]`。对应它的最终的`hidden state`用来表征整个句子，可以用于下游的分类任务
- 句子对使用一个特殊`token: [SEP]`隔开它们，将学习到的`embeddings` 加到每个`token`的`Segment embedding`上来分辨句子

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/40ce20b8d71773d6830bb3c464be4d37/35F6A2CC898F4DAA80DE6051B327A32B?ynotemdtimestamp=1565595544561" />
    <br/>
    <strong>Fig</strong>. BERT 输入表示
</p>

#### 6.3 `Pre-training BERT`

`BERT`使用两个无监督的任务：`Masked LM`以及`Next Sentence Prediction (NSP)`：

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/40ce20b8d71773d6830bb3c464be4d37/D0FA2B254640484DBE3D2B993786D51E?ynotemdtimestamp=1565595544561" />
    <br/>
    <strong>Fig</strong>. BERT 两阶段
</p>

##### 6.3.1 `Masked LM`

**`Masked`语言模型**是为了训练==深度双向语言表示向量==，随机遮住句子中的部分`token`，然后模型编码器来预测被遮住的`token`是什么。

具体做法：随即遮住每个句子中`15%`的`WordPiece tokens`

> **downside**: 造成`pre-training`和`fine-tuning`==过程不匹配==。因为==被`[MASK]`的`token`不会出现在`fine-tuning`过程中==。解决方法：将被遮住的词不只替换为`[MASK] token`。
> 1. `80%`替换为`[MASK] token`; `(my dog is hairy → my dog is [MASK])`
> 2. `10%`随机来替换一个词; `(my dog is hairy → my dog is apple)`
> 3. `10%`保持词不变. `(my dog is hairy → my dog is hairy)`
>
> 好处：编码器不知道哪些词需要预测的，哪些词是错误的，因此==被迫需要学习每一个`token`的表示向量==，因此模型可能需要更多的预训练步骤才能收敛。

##### 6.3.1 `Next Sentence Prediction (NSP)`

`NSP`是为了理解==句子间的关系==（`understanding the relationship between two sentences`）。

选取句子`A`和句子`B`作为预训练样本（`In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus.`）：

1. `50%`句子`B`是句子`A`的下一个句子（标注为`IsNext`）
2. `50%`是来自语料库的随机句子（标注为`NotNext`）

> `Pre-training data: for the pre-training corpus we use the BooksCorpus (800M words) and English Wikipedia (2,500M words).`

#### 6.4 `Fine-tuning BERT`

`BERT instead uses the` ==self-attention mechanism== `to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes` ==bidirectional cross attention== `between two sentences`.

`At the output`, ==the token representations are fed into an output layer for token-level tasks==, `such as sequence tagging or question answering`, ==and the [CLS] representation is fed into an output layer for classification==, `such as entailment or sentiment analysis.`

### 7. 词向量对比

#### 7.1 `word2vec vs. NNLM`

1. 其本质都可以看作是==语言模型==
2. 词向量只不过`NNLM`一个产物，`word2vec`虽然其本质也是语言模型，但是其==专注于词向量==本身，因此做了许多优化来提高计算效率：
> - 与`NNLM`相比，==词向量直接相加==`sum`，不再拼接，并==舍弃隐层==；
> - 考虑到`sofmax`归一化需要遍历整个词汇表，采用`hierarchical softmax`和`negative sampling`进行优化，`hierarchical softmax`实质上生成一棵==带权路径最小的哈夫曼树==，让高频词搜索路劲变小；`negative sampling`更为直接，实质上对==每一个样本中每一个词都进行负例采样==

#### 7.2 `word2vec vs. fastText`

1. 都可以==无监督学习词向量==， `fastText`训练词向量时会考虑`subword`
2. `fastText`还可以进行==有监督学习进行文本分类==，其主要特点：
> - 结构与`CBOW`类似，但学习目标是==人工标注的分类结果==
> - 采用`hierarchical softmax`对输出的分类标签建立哈==夫曼(`Huffman`)树==，样本中标签多的类别被分配短的搜寻路径；
> - 引入`N-gram`，考虑==词序特征==；
> - 引入`subword`来处理长词，处理==未登陆词==问题
3. ==输入层==：`fastText`是一个句子的每个词以及句子的`n-gram`特征，`CBOW`只是中间词的上下文
4. ==输出层==：`fastText`是预测句子的==类别标签==，而`CBOW`是预测==中间词==

#### 7.3 `word2vec vs. Glove`

1. `word2vec`是==局部语料库训练==的，其特征提取是基于滑窗的，`Glove`是基于==全局语料==的，需要事先统计==共现概率==
2. `word2vec`是无监督学习，不需要人工标注；`Glove`通常被认为是无监督学习，但实际上`Glove`还是有`label`的，即共现次数
3. `word2vec`损失函数实质上是==带权重的交叉熵==，权重固定；`Glove`的损失函数是==最小平方损失函数==，权重可以做映射变换。

> `Glove`可以被看作是更换了==目标函数==和==权重函数==的==全局==`word2vec`。

> 之前介绍的词向量均是==静态==的词向量，无法解决==一次多义==等问题。以下三种`ELMo、GPT、BERT`词向量，它们都是基于语言模型的==动态==词向量。

#### 7.4 `ELMo vs GPT vs BERT`

1. **特征提取器**：`ELMo`采用`LSTM`进行提取，`GPT`和`BERT`则采用Transformer进行提取。很多任务表明`Transformer`==特征提取能力强==于`LSTM`，`ELMo`采用`1`层静态向量+`2`层`LSTM`，多层提取能力有限，而`GPT`和`BERT`中的`Transformer`可采用多层，==并行计算能力强==。
2. **单/双向语言模型**：`GPT`采用单向语言模型，`ELMo`和`BERT`采用双向语言模型。但是`ELMo`实际上是两个单向语言模型（方向相反）的拼接，这种==融合特征的能力比`BERT`一体化融合特征方式弱==。
> 关于`Transformer`：`BERT Transformer`使用双向`self-attention`，而`GPT Transformer`使用受限制的`self-attention`，其中每个`token`只能处理其左侧的上下文。双向`Transformer`通常被称为“`Transformer encoder`”，而左侧上下文被称为“`Transformer decoder`”，`decoder`是不能获取要预测的信息的。
