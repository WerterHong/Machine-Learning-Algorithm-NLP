## 数据的标准化、归一化和正则化

[标准化-归一化-正则化-有道云笔记](http://note.youdao.com/noteshare?id=bd81eeba87ef50bd0806b7fb980aa963&sub=EF5710A82F3A4AE29A29531FC1FECB52)

### 1. 标准化

数据的标准化是将**数据按比例缩放，使之落入一个小的特定区间**。在某些比较和评价的指标处理中经常会用到，**去除数据的单位限制，将其转化为无量纲的纯数值，便于不同单位或量级的指标能够进行比较和加权**。

最常见的标准化方法就是`z-score`**标准化**，也叫零-均值标准化，这种方法给予原始数据的均值（`mean`）和标准差（`standard deviation`）进行数据的标准化。经过处理的数据符合**标准正态分布**，即均值为`0`，标准差为`1`。

```math
z _ {new} = \frac { x _ { i } - \mu } { \sigma }
```

![DTree-7](https://note.youdao.com/yws/public/resource/bd81eeba87ef50bd0806b7fb980aa963/C5BD95ADD51E47FDB6774E648A696C90?ynotemdtimestamp=1564197959361)

<p align="center">
<strong>Fig</strong>. z-score标准化
</p>

![scaling](https://note.youdao.com/yws/public/resource/bd81eeba87ef50bd0806b7fb980aa963/F6B5944E59F7467CA2DC82D05F0D15DB?ynotemdtimestamp=1564197959361)

<p align="center">
<strong>Fig</strong>. 标准化处理
</p>

- ##### 标准化的好处

> [Link](https://medium.com/@rrfd/standardize-or-normalize-examples-in-python-e3f174b65dfc)
> 1. Compare features that have different units or scales.
> 2. Standardizing tends to make the training process well behaved because the numerical condition of the optimization problems is improved.

其中最典型的就是**数据的归一化处理，即将数据统一映射到[0,1]区间上**。

### 2. 归一化

- ##### 归一化目标

1. **把数变为（0，1）之间的小数**。主要是为了数据处理方便提出来的，把数据映射到0～1范围之内处理，更加便捷快速，应该归到数字信号处理范畴之内。
2. **把有量纲表达式变为无量纲表达式**。归一化是一种简化计算的方式，即将有量纲的表达式，经过变换，化为无量纲的表达式，成为纯量。

> [Link](https://medium.com/@rrfd/standardize-or-normalize-examples-in-python-e3f174b65dfc)
>
> Here your data `x` is rescaled such that any specific `x` will now be `0 ≤ z ≤ 1`, and is done through this formula:
```math
z = \frac { x - \min ( x ) } { \max ( x ) - \min ( x ) }
```

![normalization](https://note.youdao.com/yws/public/resource/bd81eeba87ef50bd0806b7fb980aa963/26118C266B6345F2BF2C8F7C28BF5223?ynotemdtimestamp=1564197959361)

<p align="center">
<strong>Fig</strong>. 标准化处理
</p>

- ##### 归一化的好处

1. 提升模型的收敛速度

> 如下图，`$x_1$`的取值为`0-2000`，而`$x_2$`的取值为`1-5`，假如只有这两个特征，对其进行优化时，会得到一个窄长的椭圆形，导致在**梯度下降**时，**梯度的方向为垂直等高线的方向**而走之字形路线，这样会使迭代很慢，相比之下，右图的**迭代就会很快**（理解：也就是步长走多走少方向总是对的，不会走偏）

![DTree-5](https://note.youdao.com/yws/public/resource/bd81eeba87ef50bd0806b7fb980aa963/DCFDCDB3CB864A99ABB1F519ABC15CB1?ynotemdtimestamp=1564197959361)

<p align="center">
<strong>Fig</strong>. 特征缩放 Feature Scaling
</p>

2. 提升模型的精度

> 归一化的另一好处是**提高精度**，这在涉及到一些距离计算的算法时效果显著，比如算法要计算欧氏距离，上图中`$x_2$`的取值范围比较小，涉及到距离计算时其对结果的影响远比`$x_1$`带来的小，所以这就会造成精度的损失。所以归一化很有必要，他可以让**各个特征对结果做出的贡献相同**。

3. 深度学习中数据归一化可以**防止模型梯度爆炸**

> [另一种解释 Link](https://medium.com/@rrfd/standardize-or-normalize-examples-in-python-e3f174b65dfc)
> 1. Normalization makes training less sensitive to the scale of features, so we can better solve for coefficients.
> 2. The use of a normalization method will improve analysis from multiple models.
> 3. Normalizing will ensure that a convergence problem does not have a massive variance, making optimization feasible.

### 3. 正则化

正则化是一种为了**减小测试误差**的行为(有时候会增加训练误差)。

- ##### why 正则化

在构造机器学习模型时，最终目的是让模型在面对新数据的时候，可以有很好的表现。当你用比较复杂的模型比如神经网络，去拟合数据时，很容易出现**过拟合现象(训练集表现很好，测试集表现较差)**，这会导致模型的**泛化能力下降**，这时候，我们就需要使用**正则化**，**降低模型的复杂度**。

#### 3.1 正则化的几种常用方法

- ##### L1 & L2 范数

假设 `x` 是一个向量，它的 `$L^p$` 范数定义:

```math
\| x \| _ { p } = \left( \sum _ { i } \left| x _ { i } \right| ^ { p } \right) ^ { \frac { 1 } { p } }
```

在目标函数后面添加一个系数的“**惩罚项**”是正则化的常用方式，为了**防止系数过大从而让模型变得复杂**。在加了正则化项之后的目标函数为:

```math
\overline { J } ( w , b ) = J ( w , b ) + \frac { \lambda } { 2 m } \Omega ( w )
```

式中，`$\frac{\lambda}{2m}$` 是一个常数， `m` 为样本个数， `λ` 是一个超参数，用于控制正则化程度。

`$L^1$` **正则化**时，对应惩罚项为 :

```math
\Omega ( w ) = \| w \| _ { 1 } = \sum _ { i } \left| w _ { i } \right|
```

`$L^2$` **正则化**时，对应惩罚项为 :

```math
\Omega ( w ) = \| w \| _ { 2 } ^ { 2 } = \sum _ { i } w _ { i } ^ { 2 }
```

从上式可以看出，`$L^1$` **正则化**通过让原目标函数加上了**所有特征系数绝对值的和**来实现正则化，而`$L^2$` **正则化**通过让原目标函数加上了**所有特征系数的平方和**来实现正则化。

两者都是通过加上一个和项来限制参数大小， `$L^1$` 正则化就更**适用于特征选择**，而`$L^2$` 正则化更**适用于防止模型过拟合**。

#### Q1：归一化 VS 标准化 VS 正则化

**归一化**是为了**消除不同数据之间的量纲**，方便数据比较和共同处理，比如在神经网络中，归一化可以**加快训练网络的收敛性**；

**标准化**是为了方便数据的下一步处理，而进行的**数据缩放等变换**，并不是为了方便与其他数据一同处理或比较，比如数据经过**零-均值标准化**后，更利于使用**标准正态分布**的性质，进行处理；

**正则化**而是利用**先验知识**，在处理过程中引入**正则化因子**(regulator)，增加引导约束的作用，比如在++逻辑回归++中使用正则化，可**有效降低过拟合**的现象。

#### Q2：L1 & L2 正则化的区别？

- ##### 从梯度下降的角度入手，探究两者的区别

> 1. `$L^1$`正则化的目标函数:

```math
J ^ { 2 } = J + \frac { \alpha \lambda } { 2 m } \left( \left| w _ { 1 } \right| + \left| w _ { 2 } \right| \right)
```
> 在每次更新 `$w_1$` 时:

```math
\begin{array} { c } { w _ { 1 } : = w _ { 1 } - \alpha d w _ { 1 } } \\ { = w _ { 1 } - \frac { \alpha \lambda } { 2 m } \left( \frac { \partial J } { \partial w _ { 1 } } + \operatorname { sign } \left( w _ { 1 } \right) \right) } \\ { = w _ { 1 } - \frac { \alpha \lambda } { 2 m } \operatorname { sign } \left( w _ { 1 } \right) - \frac { \partial J } { \partial w _ { 1 } } } \end{array}
```
> **若 `$w_1$` 为正数，则每次更新会减去一个常数；若 `$w_1$` 为负数，则每次更新会加上一个常数，所以很容易产生特征的系数为 `0` 的情况**，特征系数为 `0` 表示该特征不会对结果有任何影响，因此`$L^1$`正则化会**让特征变得稀疏**，起到**特征选择的作用**。

> 2. `$L^2$`正则化的目标函数:

```math
\overline { J } = J + \frac { \alpha \lambda } { 2 m } \left( w _ { 1 } ^ { 2 } + w _ { 2 } ^ { 2 } \right)
```
> 在每次更新 `$w_1$` 时:

```math
\begin{array} { c } { w _ { 1 } : = w _ { 1 } - \alpha d w _ { 1 } } \\ { = w _ { 1 } - \frac { \alpha \lambda } { 2 m } \left( \frac { \partial J } { \partial w _ { 1 } } + 2 w _ { 1 } \right) } \\ { = \left( 1 - \frac { \alpha \lambda } { m } \right) w _ { 1 } - \frac { \alpha \lambda } { 2 m } \frac { \partial J } { \partial w _ { 1 } } } \end{array}
```
> 从上式可以看出每次更新时，会对特征系数进行一个**比例的缩放**而不是像`$L^1$`正则化减去一个固定值，这会让**系数趋向变小而不会变为`0`**，因此`$L^2$`正则化会**让模型变得更简单，防止过拟合**，而不会起到特征选择的作用。
