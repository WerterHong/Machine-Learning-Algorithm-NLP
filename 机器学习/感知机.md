## 感知机

- NLP-机器学习笔试面试题解析 [Github链接](https://github.com/WerterHong/Machine-Learning-Algorithm-NLP/)
- **感知机** (若公式显示错误，请点击此链接) [有道云笔记](http://note.youdao.com/noteshare?id=518d1143bc24d17b577c41c8cff3e959&sub=7D51485B1A8D453291AB3A25336D7C11)

### 1. 背景知识

#### 1.1 点到线的距离

公式中的直线方程为`Ax+By+C=0`，点`P`的坐标为`$(x_0,y_0)$`。

```math
d=\frac{Ax_0+By_0+C}{\sqrt{A^2+B^2}}
```

#### 1.2 样本到超平面的距离

我们假设超平面是`h=w⋅x+b`，其中`$w=(w_0,w_1,...w_m),x=(x_0,x_1,...x_m)$`，样本点`$x^.$`到超平面的距离如下：

```math
d=\frac{w \cdot x^.+b}{||w||}
```

### 2. 感知机模型

感知机是二类分类的==线性分类模型==，其输入是==实例的特征向量==，输出的是实例的类别，取`+1`和`-1`二值。感知机对应于输入空间（特征空间）中将实例分为正负两类的分离超平面，属于==判别模型==。

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/f6668b56b71616ff32d390c9600a03bb/9E857013582A4496BEA8F7B120EE1087?ynotemdtimestamp=1565612145775" />
    <br/>
    <strong>Fig</strong>. 感知机模型
</p>

感知机从输入空间到输出空间的模型如下：

```math
f(x)=\operatorname{sign} (w \cdot x+b)

\operatorname{sign}(x)=\left\{\begin{array}{ll}{+1} & {x \geq 0} \\ {-1} & {x<0} \end{array}\right.
```
> 感知机模型的==假设空间==是定义在特征空间中的所有线性分类模型或线性分类器，即函数集合`{f|f(x)=w·x+b}`。

### 3. 感知机学习策略

线性可分数据集`$T=\{(x_1,y_1), (x_2,y_2), ..., (x_N,y_N)\}$`，`$y_i \in \{+1, -1\}$`，超平面`S: w·x+b=0`。对所有`$y_i=+1$`的实例`i`，有`$\frac{w·x_i+b}{\|w\|}>0$`，对所有`$y_i=-1$`的实例`i`，有`$\frac{w·x_i+b}{\|w\|}<0$`。

> 误分类点`$x_i$`到超平面`S`的距离是

```math
-\frac{1}{\|w\|} y_{i}\left(w \cdot x_{i}+b\right)
```
> 超平面`S`的误分类点集合为`M`，所有误分类点到超平面`S`的总距离是

```math
-\frac{1}{\|w\|} \sum_{x_{i} \in M} y_{i}\left(w \cdot x_{i}+b\right)
```
不考虑`$\frac{1}{\|w\|}$`，==损失函数==定义如下：

```math
L(w, b)=- \sum_{x_{i} \in M} y_{i}\left(w \cdot x_{i}+b\right)
```
> 损失函数的优化目标是==期望使误分类的所有样本，到超平面的距离之和最小==

> Why 不考虑`$\frac{1}{\|w\|}$`？
>
> 1. `$\frac{1}{\|w\|}$`不影响`$y_i(w⋅x_i+b)$`正负的判断。感知机学习算法是==误分类驱动==的，这里需要注意的是所谓的“误分类驱动”指的是我们只需要判断`$-y_i(w⋅x_i+b)$`的正负来判断分类的正确与否，而`$\frac{1}{\|w\|}$`并不影响正负值的判断。
> 2. `$\frac{1}{\|w\|}$`不影响感知机学习算法的最终结果。因为感知机学习算法最终的终止条件是所有的输入都被正确分类，即不存在误分类的点，此时损失函数为`0`.

### 4. 感知机学习算法

==只有误分类的M集合里面的样本才能参与损失函数的优化==，采用随机梯度下降（`SGD`）优化如下目标函数：

```math
\min _{w, b} L(w, b)=-\sum_{x_{i} \in M} y_{i}\left(w \cdot x_{i}+b\right)
```
> 极小化过程不是一次使`M`中所有误分类点的梯度下降，而是随机选取一个误分点使其梯度下降。

损失函数`L(w, b)`的梯度由下式给出：

```math
\begin{array}{c}{\nabla_{w} L(w, b)=-\sum_{x_{i} \in M} y_{i} x_{i}} \\ {\nabla_{b} L(w, b)=-\sum_{x_{i} \in M} y_{i}}\end{array}
```
随机选取一个误分点`$(x_i, y_i)$`，对`w,b`进行更新：

```math
\begin{array}{c}{w \leftarrow w+\eta y_{i} x_{i}} \\ {b \leftarrow b+\eta y_{i}}\end{array}
```
式中(`$\eta(0< \eta <=1)$`)是步长，又称学习率(`learning rate`)。

> 通过迭代可以期待损失函数`L(w, b)`不断减小，直到为`0`。

**栗子 1**

训练数据集中正实例点`$x_1=(3, 3)^T,x_2=(4, 3)^T$`，负实例点`$x_3=(1, 1)^T$`，用感知机学习算法的原始形式求感知机模型`f(x) = sign(w·x + b)`。这里`$w = (w^{(1)}, w^{(2)})^T, x = (x^{(1)}, x^{(2)})^T$`。`η = 0`.

**Ans**：构建最优化问题：

```math
\min _{w, b} L(w, b)=-\sum_{x_{i} \in M} y_{i}\left(w \cdot x_{i}+b\right)
```
(1) 取初始值`$w_0 = 0, b_0 = 0$`

(2) 对`$x_1=(3, 3)^T, y_1 (w_0 \times x_1 + b_0) = 0$`未能被正确分类，更新`w, b`

```math
w_1 = w_0 + y_1x_1 = (3,3)^T, \quad b_1 = b_0 + y_1 = 1
```
得到线性模型

```math
w_1 \times x + b_1 = 3x^{(1)} + 3x^{(2)} + 1
```
(3) 对`$x_1, x_2$`，显然`$y_i (w_1 \times x_i + b_1) > 0$`，被正确分类，不修改`w, b`；对`$x_3$`，显然`$y_3 (w_1 \times x_3 + b_1) < 0$`，被误分类，修改`w, b`：

```math
w_2 = w_1 + y_3x_3 = (2,2)^T, \quad b_2 = b_1 + y_3 = 0
```
得到线性模型

```math
w_2 \times x + b_2 = 2x^{(1)} + 2x^{(2)}
```
如此继续，直到

```math
w_7 = (1,1)^T, \quad b_7 = -3

w_7 \times x + b_7 = x^{(1)} + x^{(2)} - 3
```
对所有数据点`$y_i (w_7 \times x_i + b_7) > 0$`，没有误分类点，损失函数达到极小。此时超平面为：

```math
x^{(1)} + x^{(2)} - 3
```

感知机模型为：

```math
f(x) = \operatorname{sign}(x^{(1)} + x^{(2)} - 3)
```

迭代过程如下表：

| 迭代次数 | 误分类点 |     `w`     | `b` |           `w·x+b`           |
|:--------:|:--------:|:-----------:|:---:|:---------------------------:|
|     0    |          |      0      |  0  |              0              |
|     1    |  `$x_1$` | `$(3,3)^T$` |  1  | `$3x^{(1)} + 3x^{(2)} + 1$` |
|     2    |  `$x_3$` | `$(2,2)^T$` |  0  |   `$2x^{(1)} + 2x^{(2)}$`   |
|     3    |  `$x_3$` | `$(1,1)^T$` |  -1 |  `$x^{(1)} + x^{(2)} - 1$`  |
|     4    |  `$x_3$` | `$(0,0)^T$` |  -2 |              -2             |
|     5    |  `$x_1$` | `$(3,3)^T$` |  -1 | `$3x^{(1)} + 3x^{(2)} - 1$` |
|     6    |  `$x_3$` | `$(2,2)^T$` |  -2 | `$2x^{(1)} + 2x^{(2)} - 2$` |
|     7    |  `$x_3$` | `$(1,1)^T$` |  -3 |  `$x^{(1)} + x^{(2)} - 3$`  |
|     8    |     0    | `$(1,1)^T$` |  -3 |  `$x^{(1)} + x^{(2)} - 3$`  |

### 4. 感知机学习算法的对偶形式

==对偶形式==的基本思想是，将`w`和`b`表示为**实例`$x_i$`和标记`$y_i$`的线性组合形式**，通过**求解其系数**而求的`w`和`b`。

上栗中通过对误分点`$(x_i,y_i)$`通过随机梯度下降不断迭代修改`w, b`，则`w, b`的增量分别为`$\alpha_iy_ix_i$`和`$\alpha_iy_i$`，这里`$\alpha_i = n_i \eta$`，得到：

```math
w = \sum^N_{i=1}\alpha_iy_ix_i

b = \sum^N_{i=1}\alpha_iy_i
```
其中，`$\alpha_i \geq 0, i = i,2,...,N$`，当`η = 1`时，表示第`i`个**实例点由于误分类而进行更新的次数**。实例点更新次数越多，意味着它距离分离超平面越近，也就越难分类。这样的实例对学习结果影响最大。

此时的感知机模型为：

```math
f(x) = \operatorname{sign}(\sum^N_{j=1}\alpha_jy_jx_j \cdot x_i + b)
```
> Why `b` 没有被线性表示？

对偶形式的==参数更新过程==：

其中`$\alpha=(\alpha_1,\alpha_2,...,\alpha_N)^T$`，输入`$\alpha, b$`初始值，学习率`$\eta(0< \eta <=1)$`。

(1) `$\alpha \leftarrow 0, \quad b \leftarrow 0$`

(2) 在训练集中选取数据(`$x_i, y_i$`)

(3) 如果`$y_{i}\left(\sum_{j=1}^{N} \alpha_{j} y_{j} x_{j} \cdot x_{i}+b\right) \leqslant 0$`

```math
\begin{aligned} & \alpha_{i} \leftarrow \alpha_{i}+\eta \\ & b \leftarrow b+\eta y_{i} \end{aligned}
```
(4) 转至`(2)`直到没有误分类数据

> 感知机学习算法的对偶形式的**栗子**见《统计学习方法》李航著 `P34`页

### 5. 感知机算法的优缺点

> ==优点==：
> 1. 感知机学习算法**简单且易于实现**
> 2. 当样本线性可分的情况下，且学习率合适时，算法具有**收敛性**
>
> ==缺点==：
> 1. 感知机只能用于**线性分类问题**，不能解决异或分类问题，但是非线性问题是普遍存在的，所以感知机的使用率较低
> 2. **收敛速度慢**。当样本线性不可分时，算法不收敛，且无法判断样本是否线性可分

### Q：感知机为什么==不能表示异或==？

Ans：异或的逻辑运算如下：

```math
\begin{array}{l}{0 \oplus 0=0,0 \oplus 1=1} \\ {1 \oplus 0=1,1 \oplus 1=0}\end{array}
```
其函数图像如下图`(d)`所示，找不到线性超平面正确分类。

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/f6668b56b71616ff32d390c9600a03bb/7BE702E717D74962B6EA5A4BE6C98340?ynotemdtimestamp=1565612145775" width=500 />
    <br/>
    <strong>Fig</strong>. 感知机 与 逻辑运算
</p>
