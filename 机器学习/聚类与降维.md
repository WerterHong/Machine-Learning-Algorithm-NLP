## 聚类与降维

- NLP-机器学习笔试面试题解析 [Github链接](https://github.com/WerterHong/Machine-Learning-Algorithm-NLP/)
- **聚类与降维** (若公式显示错误，请点击此链接) [有道云笔记](http://note.youdao.com/noteshare?id=cab55cd60a69da8c1cd91c8b7bdb7647&sub=1818550A2DEA422C8BF692D3478C6D91)

### 1. 聚类 [blog Link](https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/)

#### 1.0 距离计算

#### 1.1 `K-means`聚类

`K-means`聚类算法的==核心思想==是通过迭代**把数据对象划分到不同的簇中**，以求目标函数最小化，从而使生成的簇尽可能地紧凑和独立。

`K-means`算法针对聚类所得簇划分`$C=\{C_1,C_2,...,C_k\}$`**最小化平方误差**：

```math
E=\sum^k_{i=1}\sum_{x \in C_i}\|x-\mu_i\|^2_2

\mu_i=\frac{1}{|C_i|}\sum_{x \in C_i}x, \quad \mu_i为簇C_i的均值向量
```

> `K-means`聚类流程：
> 1. 随机选取`k`个对象作为初始的`k`个簇的**质心**
> 2. 将其余对象根据其与各个簇质心的距离分配到最近的簇；再求新形成的簇的质心
> 3. 这个迭代重定位过程不断重复，直到目标函数最小化为止

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/cab55cd60a69da8c1cd91c8b7bdb7647/AB1B771CC2574A3999902B738973950C?ynotemdtimestamp=1565855924273" width=500 />
    <br/>
    <strong>Fig</strong>. K-means 聚类
    <br/>
    <img src="https://note.youdao.com/yws/public/resource/cab55cd60a69da8c1cd91c8b7bdb7647/4AA6CE1029174D45A8D18C4A77493F3D?ynotemdtimestamp=1565855924273" width=500 />
    <br/>
    <strong>Fig</strong>. K-means 聚类结果
</p>

> 质心选择对最终结果的影响：

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/cab55cd60a69da8c1cd91c8b7bdb7647/B549C291FC834C30B242DE82C5CEBA86?ynotemdtimestamp=1565855924273" width=500 />
    <br/>
    <strong>Fig</strong>. 质心选择
</p>

`K-means`聚类算法的==优缺点==：

优点：
1. 相对高效`O(knT)`（`Its average complexity is O(knT), where k,n and T are the number of clusters, samples and iterations`）
2. 通常终止在**局部最优**，但可用全局最优技术改进。(模拟退火和遗传算法)

缺点：
1. 只适用于`numerical`类型数据，只有当质心可计算时才适用, 无法处理分类/标称数据
2. 对初始值的设置很明显，需要**事先指定簇的个数与质心**，对结果有较大影响
3. 对噪声和离群值非常敏感，无法处理**噪声**的数据
4. 不能发现**非凸形状簇**（结果图中对`Dataset 2`分类效果较差）

#### 2. 高斯混合`Gaussian Mixture`聚类

高斯混合`Gaussian Mixture`聚类采用**概率模型**来表达聚类原型。

```math
p(x)=\frac{1}{\sigma \sqrt{2 \pi}} \exp({-\frac{(x-\mu)^{2}}{2 \sigma^{2}}})
```

定义高斯混合分布：

```math
p_M(x)=\sum^k_{i=1}\alpha_i \cdot p(x|\mu_i,\sigma_i)
```
该分布共由`k`个高斯分布组成，`$\alpha_i > 0$`为相应的混合系数，`$\sum^k_{i=1}\alpha_i = 1$`

> 高斯混合聚类采用概率模型（高斯分布）对原型进行刻画，**簇划分由原型对应后验概率确定**。模型参数利用**期望最大化**`EM`算法进行迭代优化求解。

高斯混合`Gaussian Mixture`聚类过程：

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/cab55cd60a69da8c1cd91c8b7bdb7647/7279D784A4E44D3B8E24CC621835CDF5?ynotemdtimestamp=1565855924273" width=500 />
    <br/>
    <strong>Fig</strong>. 高斯混合聚类
    <br/>
    <img src="https://note.youdao.com/yws/public/resource/cab55cd60a69da8c1cd91c8b7bdb7647/E8C44D727C3D4D349D601DEF6A97D09F?ynotemdtimestamp=1565855924273" width=500 />
    <br/>
    <strong>Fig</strong>. 高斯混合聚类结果
</p>

高斯混合`Gaussian Mixture`聚类算法的==优缺点==：

优点：
1. 高斯混合模型在聚类协方差方面比`K-Means`要**灵活得多**。根据标准差参数，聚类可以采用任何椭圆形状，而不是局限于圆形
2. 根据高斯混合模型的使用概率，**每个数据点可以有多个聚类**。因此，如果一个数据点位于两个重叠的聚类的中间，通过说`X%`属于`1`类，而`y%`属于`2`类，我们可以简单地定义它的类

缺点：
1. 最终结果不能确定是**局部极小值**还是全局最小值
2. 对`Dataset 1`聚类效果好，因为其数据是正态分布，在`Dataset 2`上的效果不好

#### 3. 层次`Hierarchical`聚类

层次`hierarchical`聚类分为自顶向下（分裂`Divisive`层次聚类）或自底向上（合成`Agglomerative`层次聚类）两类。
- ==自底向上的算法==在一开始就将每个数据点视为一个单一的聚类，然后依次合并（或聚集）类，直到所有类合并成一个包含所有数据点的单一聚类。
- ==自顶向下的算法==从一个包含全部数据点的聚类开始，然后把根节点分裂为一些子聚类，每个子聚类再递归地继续往下分裂，直到出现只包含一个数据点的单节点聚类出现，即每个聚类中仅包含一个数据点。

层次`hierarchical`聚类过程：

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/cab55cd60a69da8c1cd91c8b7bdb7647/FDF1B4D21FC6401B8B4468AB8FBA8425?ynotemdtimestamp=1565855924273" width=500 />
    <br/>
    <strong>Fig</strong>. 自底向上的层次聚类
    <br/>
    <img src="https://note.youdao.com/yws/public/resource/cab55cd60a69da8c1cd91c8b7bdb7647/E81673B6D7E2481FA778B0384EB711CE?ynotemdtimestamp=1565855924273" width=500 />
    <br/>
    <strong>Fig</strong>. 层次聚类结果
</p>

层次`hierarchical`聚类有一个重要的概念叫==连接准则==（`linkage criterion`），其定义了簇之间的距离作为每个簇质点的函数，并确定在每个步骤合并/拆分哪些簇。常见的连接准则包括：

- `Ward`：最小化所有簇内差的平方和（方差）
- `Minimum or single linkage`：最小化簇的成对观测的最大距离
- `Maximum or complete linkage`：最小化簇的成对观测的最大距离
- `Average linkage`：最小化簇的所有成对观测的平均距离

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/cab55cd60a69da8c1cd91c8b7bdb7647/55906E6BD80A49A8A6BE8E86F9346F0E?ynotemdtimestamp=1565855924273" width=500 />
    <br/>
    <strong>Fig</strong>. linkage criterion
</p>

> 通过添加==连通性约束==（点只能与`n(=5)`个最近的点聚类），层次`hierarchical`聚类可以对非球状数据进行聚类。

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/cab55cd60a69da8c1cd91c8b7bdb7647/70D486D6047E4CC0BDC8D99D75792490?ynotemdtimestamp=1565855924273" width=500 />
    <br/>
    <strong>Fig</strong>. 连通性约束
</p>

层次`hierarchical`聚类算法的==优缺点==：

优点：
1. 可解释性好（如当需要创建一种分类法时）
2. 产生高质量的聚类
3. 解决**非凸（`non-convex`）数据**

缺点：
1. **时间复杂度高**`$o(m^3)$`，改进后的算法`$o(m^2logm)$`，`m`为点的个数
2. 贪心算法的缺点，一步错步步错


#### 4. 密度`Hensity`聚类

密度`Hensity`聚类假设聚类结构能够通过样本分布的紧密程度确定。典型是`DBSCAN`算法，它基于“邻域”（`neighborhood`）参数（`$\epsilon,MinPts$`）**刻画样本分布的紧密程度**。

参数定义：
- *`$\epsilon-$`邻域*：对于`$x_j \in D$`，其`$\epsilon-$`邻域包含样本集中与`$x_j$`的距离不大于`$\epsilon$`的样本
- *核心对象*（`core object`）：若`$x_j$`的`$\epsilon-$`邻域至少包含`MinPts`个样本，则`$x_j$`是一个核心对象
- *密度直达*（`directly density-reachable`）：若`$x_j$`在`$x_i$`的`$\epsilon-$`邻域内，且`$x_i$`是核心对象，则称`$x_j$`由`$x_i$`密度直达
- *密度可达*（`density-reachable`）：对`$x_i$`和`$x_j$`，若存在样本序列`$p_1,p_2,…,p_n$`，其中`$p_1=x_i, p_n=x_j$`且`$p_{i+1}$`由`$p_i$`密度直达，则称`$x_j$`由`$x_i$`密度可达
- *密度相连*（`density-connected`）：对`$x_i$`和`$x_j$`，若存在`$x_k$`使得`$x_i$`与`$x_j$`均由`$x_k$`密度可达，则称`$x_j$`由`$x_i$`密度相连

`DBSCAN`聚类==算法过程==：

1. 根据参数(`$\epsilon,MinPts$`)确定各样本的邻域和核心对象集合`$\Omega$`
2. 从`$\Omega$`中随机选取一个核心对象作为种子（`seed`），找出由它**密度可达**的所有样本，构成第一个聚类簇`$C_1$`
3. 将`$C_1$`中包含的核心对象从`$\Omega$`中去除得到更新后的`$\Omega$`
4. 从更新后的`$\Omega$`随机选取一个核心对象生成下一个聚类簇
5. 重复步骤`2, 3`，直至`$\Omega$`为空

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/cab55cd60a69da8c1cd91c8b7bdb7647/55B54B689D464A8EB54901634B08E78B?ynotemdtimestamp=1565855924273" width=300 />
    <br/>
    <strong>Fig</strong>. DBSCAN 聚类
    <br/>
    <img src="https://note.youdao.com/yws/public/resource/cab55cd60a69da8c1cd91c8b7bdb7647/5E4F5798E8EA4AF0AD2FD04BA9DA8016?ynotemdtimestamp=1565855924273" width=500 />
    <br/>
    <strong>Fig</strong>. DBSCAN 聚类结果
</p>

`DBscan`聚类算法参数对结果的影响：

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/cab55cd60a69da8c1cd91c8b7bdb7647/8240C9A853484272B6EC824D44740716?ynotemdtimestamp=1565855924273" width=500 />
    <br/>
    <strong>Fig</strong>. 参数对结果的影响
</p>

`DBSCAN`聚类算法的==优缺点==：

优点：
1. 对**噪声不敏感**
2. 能发现**任意形状的聚类**

缺点：
1. 聚类的结果与**参数有很大的关系**
2. `DBSCAN`聚类算法用固定参数识别聚类，但当聚类的稀疏程度不同时，相同的判定标准可能会**破坏聚类的自然结构**，即较稀的聚类会被划分为多个类或密度较大且离得较近的类会被合并成一个聚类

### 2. 降维

#### 2.1 `t-SNE`

#### 2.2 `PCA`