## 聚类与降维

- NLP-机器学习笔试面试题解析 [Github链接](https://github.com/WerterHong/Machine-Learning-Algorithm-NLP/)
- **聚类与降维** (若公式显示错误，请点击此链接) [有道云笔记](http://note.youdao.com/noteshare?id=6ce267da8700d05f5499f0370c61728c&sub=8698920FF34649FA9B5C039CFC691CE9)

### 1. 聚类 [blog Link](https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/)

#### 1.0 距离计算

距离度量函数`dist(,)`需要满足：

- [x] 非负性：`$dist(,) \geq 0$`
- [x] 同一性：`$dist(,) = 0$`当且仅当`$x_i=x_j$`
- [x] 对称性：`$dist(x_i, x_j) = dist(x_j, x_i)$`
- [x] 直递性：`$dist(x_i, x_j) \leq dist(x_i, x_k)+dist(x_k, x_j)$`

给定样本`$x_i=(x_{i1}; x_{i2}; ...; x_{in})$`与`$x_i=(x_{j1}; x_{j2}; ...; x_{jn})$`，最常用的是“==闵可夫斯基距离==”（`Minkowski distance`）：

```math
dist_{mk}(x_i,x_j)=\left( \sum^n_{u=1}|x_{iu}-x_{ju}|^p \right)^{\frac{1}{p}}
```
对`p >= 1`，上式满足距离度量的基本性质。当`p=2`时，闵可夫斯基距离即==欧式距离==（`Euclidean distance`）:

```math
dist_{ed}(x_i,x_j)=\|x_i-x_j\|_2=\sqrt {\sum^n_{u=1}|x_{iu}-x_{ju}|^2}
```
当`p=1`时，闵可夫斯基距离即==曼哈顿距离==（`Manhattan distance`）:

```math
dist_{man}(x_i,x_j)=\|x_i-x_j\|_1=\sum^n_{u=1}|x_{iu}-x_{ju}|
```
当`p`趋于无穷时，闵可夫斯基距离即==切比雪夫==。

当样本空间中不同属性的重要性不同时，可使用“==加权距离==”（`weighted distance`），以加权闵可夫斯基距离为例：

```math
dist_{wmk}(x_i,x_j)=\left( w_1 \cdot |x_{i1}-x_{j1}|^p + \cdots + w_n \cdot |x_{in}-x_{jn}|^p \right)^{\frac{1}{p}}
```
其中权重`$w_i \geq 0, (i=1,2,...,n)$`表征不同属性的重要性，通常`$\sum^n_{i=1} w_i =1$`

#### 1.1 `K-means`聚类

`K-means`聚类算法的==核心思想==是通过迭代**把数据对象划分到不同的簇中**，以求目标函数最小化，从而使生成的簇尽可能地紧凑和独立。

`K-means`算法针对聚类所得簇划分`$C=\{C_1,C_2,...,C_k\}$`**最小化平方误差**：

```math
E=\sum^k_{i=1}\sum_{x \in C_i}\|x-\mu_i\|^2_2

\mu_i=\frac{1}{|C_i|}\sum_{x \in C_i}x, \quad \mu_i为簇C_i的均值向量
```

> `K-means`聚类流程：
> 1. 随机选取`k`个对象作为初始的`k`个簇的**质心**
> 2. 将其余对象根据其与各个簇质心的距离分配到最近的簇；再求新形成的簇的质心
> 3. 这个迭代重定位过程不断重复，直到目标函数最小化为止

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/cab55cd60a69da8c1cd91c8b7bdb7647/AB1B771CC2574A3999902B738973950C?ynotemdtimestamp=1565855924273" width=500 />
    <br/>
    <strong>Fig</strong>. K-means 聚类
    <br/>
    <img src="https://note.youdao.com/yws/public/resource/cab55cd60a69da8c1cd91c8b7bdb7647/4AA6CE1029174D45A8D18C4A77493F3D?ynotemdtimestamp=1565855924273" width=500 />
    <br/>
    <strong>Fig</strong>. K-means 聚类结果
</p>

> 质心选择对最终结果的影响：

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/cab55cd60a69da8c1cd91c8b7bdb7647/B549C291FC834C30B242DE82C5CEBA86?ynotemdtimestamp=1565855924273" width=500 />
    <br/>
    <strong>Fig</strong>. 质心选择
</p>

`K-means`聚类算法的==优缺点==：

优点：
1. 相对高效`O(knT)`（`Its average complexity is O(knT), where k,n and T are the number of clusters, samples and iterations`）
2. 通常终止在**局部最优**，但可用全局最优技术改进。(模拟退火和遗传算法)

缺点：
1. 只适用于`numerical`类型数据，只有当质心可计算时才适用, 无法处理分类/标称数据
2. 对初始值的设置很明显，需要**事先指定簇的个数与质心**，对结果有较大影响
3. 对噪声和离群值非常敏感，无法处理**噪声**的数据
4. 不能发现**非凸形状簇**（结果图中对`Dataset 2`分类效果较差）

#### 2. 高斯混合`Gaussian Mixture`聚类

高斯混合`Gaussian Mixture`聚类采用**概率模型**来表达聚类原型。

```math
p(x)=\frac{1}{\sigma \sqrt{2 \pi}} \exp({-\frac{(x-\mu)^{2}}{2 \sigma^{2}}})
```

定义高斯混合分布：

```math
p_M(x)=\sum^k_{i=1}\alpha_i \cdot p(x|\mu_i,\sigma_i)
```
该分布共由`k`个高斯分布组成，`$\alpha_i > 0$`为相应的混合系数，`$\sum^k_{i=1}\alpha_i = 1$`

> 高斯混合聚类采用概率模型（高斯分布）对原型进行刻画，**簇划分由原型对应后验概率确定**。模型参数利用**期望最大化**`EM`算法进行迭代优化求解。

高斯混合`Gaussian Mixture`聚类过程：

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/cab55cd60a69da8c1cd91c8b7bdb7647/7279D784A4E44D3B8E24CC621835CDF5?ynotemdtimestamp=1565855924273" width=500 />
    <br/>
    <strong>Fig</strong>. 高斯混合聚类
    <br/>
    <img src="https://note.youdao.com/yws/public/resource/cab55cd60a69da8c1cd91c8b7bdb7647/E8C44D727C3D4D349D601DEF6A97D09F?ynotemdtimestamp=1565855924273" width=500 />
    <br/>
    <strong>Fig</strong>. 高斯混合聚类结果
</p>

高斯混合`Gaussian Mixture`聚类算法的==优缺点==：

优点：
1. 高斯混合模型在聚类协方差方面比`K-Means`要**灵活得多**。根据标准差参数，聚类可以采用任何椭圆形状，而不是局限于圆形
2. 根据高斯混合模型的使用概率，**每个数据点可以有多个聚类**。因此，如果一个数据点位于两个重叠的聚类的中间，通过说`X%`属于`1`类，而`y%`属于`2`类，我们可以简单地定义它的类

缺点：
1. 最终结果不能确定是**局部极小值**还是全局最小值
2. 对`Dataset 1`聚类效果好，因为其数据是正态分布，在`Dataset 2`上的效果不好

#### 3. 层次`Hierarchical`聚类

层次`hierarchical`聚类分为自顶向下（分裂`Divisive`层次聚类）或自底向上（合成`Agglomerative`层次聚类）两类。
- ==自底向上的算法==在一开始就将每个数据点视为一个单一的聚类，然后依次合并（或聚集）类，直到所有类合并成一个包含所有数据点的单一聚类。
- ==自顶向下的算法==从一个包含全部数据点的聚类开始，然后把根节点分裂为一些子聚类，每个子聚类再递归地继续往下分裂，直到出现只包含一个数据点的单节点聚类出现，即每个聚类中仅包含一个数据点。

层次`hierarchical`聚类过程：

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/cab55cd60a69da8c1cd91c8b7bdb7647/FDF1B4D21FC6401B8B4468AB8FBA8425?ynotemdtimestamp=1565855924273" width=500 />
    <br/>
    <strong>Fig</strong>. 自底向上的层次聚类
    <br/>
    <img src="https://note.youdao.com/yws/public/resource/cab55cd60a69da8c1cd91c8b7bdb7647/E81673B6D7E2481FA778B0384EB711CE?ynotemdtimestamp=1565855924273" width=500 />
    <br/>
    <strong>Fig</strong>. 层次聚类结果
</p>

层次`hierarchical`聚类有一个重要的概念叫==连接准则==（`linkage criterion`），其定义了簇之间的距离作为每个簇质点的函数，并确定在每个步骤合并/拆分哪些簇。常见的连接准则包括：

- `Ward`：最小化所有簇内差的平方和（方差）
- `Minimum or single linkage`：最小化簇的成对观测的最大距离
- `Maximum or complete linkage`：最小化簇的成对观测的最大距离
- `Average linkage`：最小化簇的所有成对观测的平均距离

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/cab55cd60a69da8c1cd91c8b7bdb7647/55906E6BD80A49A8A6BE8E86F9346F0E?ynotemdtimestamp=1565855924273" width=500 />
    <br/>
    <strong>Fig</strong>. linkage criterion
</p>

> 通过添加==连通性约束==（点只能与`n(=5)`个最近的点聚类），层次`hierarchical`聚类可以对非球状数据进行聚类。

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/cab55cd60a69da8c1cd91c8b7bdb7647/70D486D6047E4CC0BDC8D99D75792490?ynotemdtimestamp=1565855924273" width=500 />
    <br/>
    <strong>Fig</strong>. 连通性约束
</p>

层次`hierarchical`聚类算法的==优缺点==：

优点：
1. 可解释性好（如当需要创建一种分类法时）
2. 产生高质量的聚类
3. 解决**非凸（`non-convex`）数据**

缺点：
1. **时间复杂度高**`$o(m^3)$`，改进后的算法`$o(m^2logm)$`，`m`为点的个数
2. 贪心算法的缺点，一步错步步错


#### 4. 密度`Hensity`聚类

密度`Hensity`聚类假设聚类结构能够通过样本分布的紧密程度确定。典型是`DBSCAN`算法，它基于“邻域”（`neighborhood`）参数（`$\epsilon,MinPts$`）**刻画样本分布的紧密程度**。

参数定义：
- *`$\epsilon-$`邻域*：对于`$x_j \in D$`，其`$\epsilon-$`邻域包含样本集中与`$x_j$`的距离不大于`$\epsilon$`的样本
- *核心对象*（`core object`）：若`$x_j$`的`$\epsilon-$`邻域至少包含`MinPts`个样本，则`$x_j$`是一个核心对象
- *密度直达*（`directly density-reachable`）：若`$x_j$`在`$x_i$`的`$\epsilon-$`邻域内，且`$x_i$`是核心对象，则称`$x_j$`由`$x_i$`密度直达
- *密度可达*（`density-reachable`）：对`$x_i$`和`$x_j$`，若存在样本序列`$p_1,p_2,…,p_n$`，其中`$p_1=x_i, p_n=x_j$`且`$p_{i+1}$`由`$p_i$`密度直达，则称`$x_j$`由`$x_i$`密度可达
- *密度相连*（`density-connected`）：对`$x_i$`和`$x_j$`，若存在`$x_k$`使得`$x_i$`与`$x_j$`均由`$x_k$`密度可达，则称`$x_j$`由`$x_i$`密度相连

`DBSCAN`聚类==算法过程==：

1. 根据参数(`$\epsilon,MinPts$`)确定各样本的邻域和核心对象集合`$\Omega$`
2. 从`$\Omega$`中随机选取一个核心对象作为种子（`seed`），找出由它**密度可达**的所有样本，构成第一个聚类簇`$C_1$`
3. 将`$C_1$`中包含的核心对象从`$\Omega$`中去除得到更新后的`$\Omega$`
4. 从更新后的`$\Omega$`随机选取一个核心对象生成下一个聚类簇
5. 重复步骤`2, 3`，直至`$\Omega$`为空

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/cab55cd60a69da8c1cd91c8b7bdb7647/55B54B689D464A8EB54901634B08E78B?ynotemdtimestamp=1565855924273" width=300 />
    <br/>
    <strong>Fig</strong>. DBSCAN 聚类
    <br/>
    <img src="https://note.youdao.com/yws/public/resource/cab55cd60a69da8c1cd91c8b7bdb7647/5E4F5798E8EA4AF0AD2FD04BA9DA8016?ynotemdtimestamp=1565855924273" width=500 />
    <br/>
    <strong>Fig</strong>. DBSCAN 聚类结果
</p>

`DBscan`聚类算法参数对结果的影响：

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/cab55cd60a69da8c1cd91c8b7bdb7647/8240C9A853484272B6EC824D44740716?ynotemdtimestamp=1565855924273" width=500 />
    <br/>
    <strong>Fig</strong>. 参数对结果的影响
</p>

`DBSCAN`聚类算法的==优缺点==：

优点：
1. 对**噪声不敏感**
2. 能发现**任意形状的聚类**

缺点：
1. 聚类的结果与**参数有很大的关系**
2. `DBSCAN`聚类算法用固定参数识别聚类，但当聚类的稀疏程度不同时，相同的判定标准可能会**破坏聚类的自然结构**，即较稀的聚类会被划分为多个类或密度较大且离得较近的类会被合并成一个聚类

### 2. 降维

#### 2.1 `k`近邻学习

`k`近邻（`k-Kearest Neighbor, KNN`）是一种监督学习方法。其主要思想就是根据距离相近的邻居类别，来判定自己的所属类别。在分类任务中采用“投票法”，在回归任务中采用“平均法”。

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/cab55cd60a69da8c1cd91c8b7bdb7647/D1AD77A4538C47CA9DBCF39485C9AA3B?ynotemdtimestamp=1565870564599" width=400 />
    <br/>
    <strong>Fig</strong>. KNN 算法示例
</p>

上图中，当`k = 3`时，目标样本"星星"最靠近的样本有两个属于`class b`，一个属于`class A`，按照投票法，"星星"属于`class B`。

`k`近邻学习算法==优缺点==：

优点：
1. 思想简单，易于理解，易于实现，无需估计参数，无需训练
2. 适合对稀有事物进行分类
3. 特别适合于**多分类问题**

缺点：
1. 懒惰算法，进行分类时**计算量大**，要扫描全部训练样本计算距离，内存开销大，评分慢
2. 当**样本不平衡**时，如其中一个类别的样本较大，可能会导致对新样本计算近邻时，大容量样本占大多数，**影响分类效果**
3. 可**解释性较差**，无法给出决策树那样的规则

> 在高维情况下会出现数据样本稀疏、距离计算困难等问题，被称为“==维数灾难==”（`curse of dimensionality`）。缓解维数灾难的重要途径是==降维==（`dimension reduction`），即**通过某种数学变换将原始高维属性空间转变为一个低维“子空间”**。

#### 2.2 `PCA`降维

==主成分分析==（`Principal Component Analysis, PCA`）是最常用的一种降维方法。`PCA`的主要思想是将`n`维特征映射到`k`维上，这`k`维是**全新的正交特征也被称为主成分**，是在原有`n`维特征的基础上重新构造出来的`k`维特征。

> Q：如何得到这些包含==最大差异性的主成分方向==？
>
> Ans：通过计算数据矩阵的协方差矩阵，然后得到协方差矩阵的特征值特征向量，选择**特征值最大(即方差最大)的`k`个特征所对应的特征向量组成的矩阵**。这样就可以将数据矩阵转换到新的空间当中，实现数据特征的降维。
>
> 得到协方差矩阵的特征值特征向量有两种方法：**特征值分解**协方差矩阵、**奇异值分解**协方差矩阵。

- 输入：数据集`$X=\{x_1,x_2,...,x_n\}$`，需要降到`k`维。==基于`SVD`分解协方差矩阵==实现PCA算法：

1. 中心化，即每一位特征减去各自的平均值
2. 计算散度矩阵（协方差矩阵*`(n-1)`）`$XX^T$`
3. 通过`SVD`计算散度矩阵的特征值与特征向量
4. 对特征值从大到小排序，选择其中最大的`k`个。然后将其对应的`k`个特征向量分别作为列向量组成特征向量矩阵
5. 将数据转换到`k`个特征向量构建的新空间中

##### `SVD`分解矩阵原理

奇异值分解是一个能适用于任意矩阵的一种分解的方法，对于任意矩阵`A`总是存在一个奇异值分解：

```math
A_{m \times n}=U_{m \times m} \Sigma_{m \times n} V_{n \times n}^{T}
```
`U`里面的正交向量被称为**左奇异向量**。`$\Sigma$`除了对角线其它元素都为`0`，对角线上的元素称为**奇异值**。`$V^T$`里面的正交向量被称为**右奇异值向量**。左奇异矩阵可以用于对**行数的压缩**；右奇异矩阵可以用于对**列(即特征维度)的压缩**。

`SVD`分解矩阵`A`的步骤：

1. 求`$XX^T$`的特征值和特征向量，用单位化的特征向量构成`U`
2. 求`$X^TX$`的特征值和特征向量，用单位化的特征向量构成`V`
3. 将`$XX^T$`或者`$X^TX$`的特征值求平方根，然后构成`$\Sigma$`

`PCA`==缺点==分析：

1. `PCA`是一个线性降维方法，对于**非线性问题**，`PCA`则无法发挥其作用
2. `PCA`需要选择主成分个数，但是没有一个很好的界定准则来确定**最佳主成分个数**
3. **可解释性差**。多数情况下，难以解释PCA所保持的主成分分量的意义
4. `PCA`将所有的样本作为一个整体对待，去寻找一个均方误差最小意义下的最优线性映射，而**忽略了类别属性**，而它所忽略的投影方向有可能刚好包含了重要的可分类信息


> `PCA`降维前，应==归一化数据==。`PCA`原理见 [知乎 blog](https://www.zhihu.com/question/41120789)。

#### 2.3 `t-SNE`

`t-SNE, t-distributed Stochastic Neighbor Embedding`将数据点之间的**相似度转换为概率**。原始高维空间中的相似度由**高斯联合概率**表示，嵌入空间的相似度**由学生`t`-分布**表示。

> 与高斯分布相比`t`分布有较长的尾部，这有助于**数据点在二维空间中更均匀地分布**。

##### 2.3.1 `SNE` 原理

`SNE`是将**欧几里得距离转换为条件概率来表达点与点之间地相似度**。主要包括两个步骤：

1. `SNE`构建一个高维样本之间的概率分布，使得**相似的样本有更高的概率被选择**，而不相似的对象有较低的概率被选择
2. `SNE`在低维空间里构建这些点的概率分布，使得这**两个概率分布之间尽可能地相似**

`SNE`求解步骤：

**Step 1**：将数据点之间的高维欧几里得距离转换为表示相似性的条件概率，高维样本之间的相似度（概率表示）：

```math
p_{j|i}=\frac{\exp(-\|x_i-x_j\|^2/(2\sigma^2_i))}{\sum_{k \neq i}\exp(-\|x_i-x_k\|^2/(2\sigma^2_i))}，\quad p_{i|i}=0
```
**Step 2**：低维下的样本表示（高斯分布为方差`$\frac{1}{\sqrt{2}}$`）：

```math
q_{j|i}=\frac{\exp(-\|y_i-y_j\|^2)}{\sum_{k \neq i}\exp(-\|y_i-y_k\|^2)}，\quad q_{i|i}=0
```
**Step 3**：如果降维效果好，局部特征保留完整，那么`$p_{j|i}=q_{j|i}$`，优化两个分布之间的距离，利用`KL`散度作为目标函数有:

```math
C=\sum_i \operatorname{KL}(P_i\|Q_i)=\sum_i \sum_j p_{j|i}\log \frac{p_{j|i}}{q_{j|i}}
```
> ==`KL`散度==具有**不对称性**，在低维映射不同的距离对应不同的惩罚权重是不同的，具体来说：距离较远的两个点来表达距离较近的两个点会产生更大的cost，相反，用较近的两个点来表达较远的两个点产生的cost相对较小。==因此`SNE`倾向于保留数据中的局部特征==。

**Step 4**：不同的点具有不同的`$\sigma_i$`，`$P_i$`的熵(`entropy`)会随着`$\sigma_i$`的增加而增加。`SNE`使用==困惑度==(`perplexity`)的概念，用**二分搜索**的方式来寻找一个最佳的`$\sigma$`。其中困惑度指:

```math
Perp(P_i)=2^{H(P_i)}
```
这里的`$H(P_i)$`是`$P_i$`的熵，即:

```math
H(P_i)=-\sum_j p_{j|i} \log (p_{j|i})
```
困惑度可以解释为一个点附近的有效近邻点个数。`SNE`对困惑度的调整比较有**鲁棒性**，通常选择`5-50`之间，给定之后，使用二分搜索的方式寻找合适的`$\sigma$`。梯度计算：

```math
\frac{\partial C}{\partial y_i}=2\sum_j (p_{j|i}-q_{j|i}+p_{i|j}-q_{i|j})(y_i-y_j)
```

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/cab55cd60a69da8c1cd91c8b7bdb7647/10A54A407BA14CCBA9B40451877DE0F3?ynotemdtimestamp=1566028096346" />
    <br/>
    <strong>Fig</strong>. 困惑度对 t-SNE 的影响
</p>

`SNE`==缺点==：
1. 很难优化
2. 存在“`crowding problem`”（拥挤问题）

`t-ANE`==改进==`SNE`：
1. 使用对称版的`SNE`，简化梯度公式
2. 低维空间下，使用学生`t`分布替代高斯分布表达两点之间的相似度

##### 2.3.2 对称`Symmetric`SNE

`Symmetric SNE`中`KL`散度使用联合概率分布来替换条件概率分布，目标函数为

```math
C=\sum \operatorname{KL}(P\|Q)=\sum_i \sum_j p_{ji}\log \frac{p_{ji}}{q_{ji}}
```
`Symmetric SNE`假设了对于任意`$i,p_{ij}=p_{ji},q_{ij}=q_{ji}$`，因此概率分布可以改写为：

```math
p_{ji}=\frac{\exp(-\|x_i-x_j\|^2/(2\sigma^2_i))}{\sum_{k \neq l}\exp(-\|x_k-x_l\|^2/(2\sigma^2_i))}，\quad p_{ii}=0

q_{ji}=\frac{\exp(-\|y_i-y_j\|^2)}{\sum_{k \neq l}\exp(-\|y_k-y_l\|^2)}，\quad q_{ii}=0
```
会引入**异常值**，将联合概率分布定义修正为`$p_{ij}=\frac{p_{i|j}+p_{j|i}}{2}$`，这保证了`$\sum_j p_{ij}>\frac{1}{2n}$`，使得每个点对于`cost`都会有一定的贡献。梯度计算：

```math
\frac{\partial C}{\partial y_i}=4\sum_j (p_{ij}-q_{ij})(y_i-y_j)
```

>
> <p align="center">
>     <img src="https://note.youdao.com/yws/public/resource/cab55cd60a69da8c1cd91c8b7bdb7647/5876C052FB604D32A848DE42BCB7164B?ynotemdtimestamp=1566028096346" />
>     <br/>
>     <strong>Fig</strong>. 高斯分布 vs. t 分布
> </p>
>
> ==`crowding`问题==：
>
> 想象在一个三维的球里面有均匀分布的点，如果把这些点投影到一个二维的圆上一定会有很多点是重合的。所以在二维的圆上想尽可能表达出三维里的点的信息，把由于投影所重合的点用不同的距离（差别很小）表示，这样就会占用原来在那些距离上的点，原来那些点会被赶到更远一点的地方。**`t`分布是长尾的，意味着距离更远的点依然能给出和高斯分布下距离小的点相同的概率值，从而达到高维空间和低维空间对应的点概率相同的目的**。
>
> `t-SNE`在低维空间下使用更重**长尾分布的学生`t`分布来避免`crowding`问题和优化问题**。

##### 2.3.3 t-SNE

在`t-SNE`中，`$q_{ij}$`的定义为：

```math
q_{ji}=\frac{(1+\|y_i-y_j\|^2)^{-1}}{\sum_{k \neq l}(1+\|y_k-y_l\|^2)^{-1}},\quad q_{ii}=0
```
梯度计算：

```math
\frac{\partial C}{\partial y_i}=4\sum_j (p_{ij}-q_{ij})(y_i-y_j)(1+\|y_k-y_l\|^2)^{-1}
```
> 对于较大相似度的点，t分布在低维空间中的距离需要稍微小一点；而对于低相似度的点，t分布在低维空间中的距离需要更远。

<p align="center">
     <img src="https://note.youdao.com/yws/public/resource/cab55cd60a69da8c1cd91c8b7bdb7647/D2FD4E33DA1D42828F1658F8B50B35B6?ynotemdtimestamp=1566028096346" height=350 />
     <br/>
     <strong>Fig</strong>. t 长尾分布
</p>

`t-SNE`的==梯度更新优势==：

1. 对于不相似的点，用一个较小的距离会产生较大的梯度来让这些点排斥开来
2. 这种排斥又不会无限大(梯度中分母),避免不相似的点距离太远。

> Q：From [`Laurens van der Maaten`](https://lvdmaaten.github.io/tsne/)
