## 朴素贝叶斯

[NLP-机器学习笔试面试题解析]Github链接(https://github.com/WerterHong/Machine-Learning-Algorithm-NLP/tree/master/机器学习算法/)

朴素贝叶斯（Naïve Bayes）法是基于贝叶斯定理与特征条件独立假设的分类方法。
> 朴素：特征条件独立
>
> 贝叶斯：基于贝叶斯定理：`$p(y | x)=\frac{p(x | y) p(y)}{p(x)}$`

朴素贝叶斯法实际上学习到生成数据的机制，因为其学习的是一个**联合概率分布**，所以属于**生成模型**。

### 1. 朴素贝叶斯法

假设输入空间`$\mathcal{X} \subseteq R^{n}$`为`n`维向量的集合，输出空间为类标记的集合`$\mathcal{Y} \subseteq\left\{c_{1}, c_{2}, \cdots, c_{K}\right\}$`.输入特征向量`$x \in \mathcal{X}$`,输出类标记`$y \in \mathcal{Y}$`.`X`是定义在输出空间`$\mathcal{X}$`上的随机变量，`Y`是定义在输出空间`$\mathcal{Y}$`上的随机变量.`P(X,Y)`是`X`和`Y`上的联合概率分布。训练数据集为
```math
T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}
```
由`P(X,Y)`独立同分布产生。

朴素贝叶斯法通过训练数据集学习联合概率分布`P(X,Y)`。
- 学习先验概率分布以及条件概率分布,得到联合概率分布`P(X,Y)`
> 先验概率分布
> ```math
> P\left(Y=c_{k}\right), k=1,2,3, \cdots, K
> ```
> 条件概率分布
> ```math
> P\left(X=x | Y=c_{k}\right)=P\left(X^{(1)}=x^{1}, \cdots, X^{(n)}=x^{(n)} | Y=c_{k}\right), k=1,2, \cdots, K
> ```
> - 条件概率分布`$P\left(X=x | Y=c_{k}\right)$`有指数级数量的参数，其估计实际是不可行的.

朴素贝叶斯法对条件概率分布做了**条件独立性假设**。假设**用于分类的特征在类稳定的条件下都是条件独立的**，模型包含的条件概率的数量大大减少，学习与预测大为简化，但是会损失准确率。条件独立性假设是：

```math
\begin{array}{}
P\left(X=x | Y=c_{k}\right)=P\left(X^{(1)}=x^{(1)}, \cdots, X^{(n)} | Y=c_{k}\right)=\prod_{j=1}^{n} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right)
\tag{1.1}
\end{array}
```

朴素贝叶斯方法在分类时，对给定的输入`x`, 通过学习到的模型计算后验概率分`$P(Y=c_k|X=x)$` 将后验概率最大的类作为`x`类的输出，后验概率计算可以依据贝叶斯定理进行：

```math
\begin{array}{}
P\left(Y=c_{k} | X=x\right)=\frac{P\left(X=x | Y=c_{k}\right) P\left(Y=c_{k}\right)}{\sum_{k} P\left(X=x | Y=c_{k}\right) P\left(Y=c_{k}\right)}
\tag{1.2}
\end{array}
```
将1.1代入1.2式，得到：
```math
\begin{array}{}
P\left(Y=c_{k} | X=x\right)=\frac{P\left(Y=c_{k}\right) \Pi_{j} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right)}{\sum_{k} P\left(Y=c_{k}\right) \Pi_{j} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right)}, k=1,2, \cdots, K
\tag{1.3}
\end{array}
```
式(1.3)是朴素贝叶斯分类的基本公式。朴素贝叶斯分类器可表示为：
```math
\begin{array}{}
y=f(x)=\arg \max _{c k} \frac{P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right)}{\sum_{k} P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right)}
\tag{1.4}
\end{array}
```
简化得到最终的**贝叶斯分类器的决策函数**为：
```math
\begin{array}{}
y=\arg \max _{c k} P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right)
\tag{1.5}
\end{array}
```

#### 后验概率最大化

详情见《统计学习方法》4.1.2  P48 以及《机器学习》

### 2. 参数估计(极大似然函数)

[详情见极大似然估计](https://github.com/WerterHong/Machine-Learning-Algorithm-NLP/tree/master/机器学习算法/极大似然估计.md)