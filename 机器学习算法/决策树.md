# 决策树

[NLP-机器学习笔试面试题解析]Github链接(https://github.com/WerterHong/Machine-Learning-Algorithm-NLP/tree/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/)


## 什么是决策树

决策树(decision tree)是一种基本的分类与回归方法。决策树是用样本的属性作为结点，用属性的取值作为分支的树结构。 

决策树生成算法：
![决策树生成算法](https://img-blog.csdn.net/20180724162804286?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FtMjkwMzMzNTY2/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

决策树的根结点是所有样本中信息量最大的属性。树的中间结点是该结点为根的子树所包含的样本子集中信息量最大的属性。决策树的叶结点是样本的类别值。决策树是一种知识表示形式，它是对所有样本数据的高度概括决策树能准确地识别所有样本的类别，也能有效地识别新样本的类别。

## 特征选择

| 编号 | 色泽 | 根蒂 | 敲声 | 纹理 | 脐部 | 触感 | 好瓜 |
|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|
|   1  | 青绿 | 蜷缩 | 浊响 | 清晰 | 凹陷 | 硬滑 |  是  |
|   2  | 乌黑 | 蜷缩 | 沉闷 | 清晰 | 凹陷 | 硬滑 |  是  |
|   3  | 乌黑 | 蜷缩 | 浊响 | 清晰 | 凹陷 | 硬滑 |  是  |
|   4  | 青绿 | 蜷缩 | 沉闷 | 清晰 | 凹陷 | 硬滑 |  是  |
|   5  | 浅白 | 蜷缩 | 浊响 | 清晰 | 凹陷 | 硬滑 |  是  |
|   6  | 青绿 | 稍蜷 | 浊响 | 清晰 | 稍凹 | 软粘 |  是  |
|   7  | 乌黑 | 稍蜷 | 浊响 | 稍糊 | 稍凹 | 软粘 |  是  |
|   8  | 乌黑 | 稍蜷 | 浊响 | 清晰 | 稍凹 | 硬滑 |  是  |
|   9  | 乌黑 | 稍蜷 | 沉闷 | 稍糊 | 稍凹 | 硬滑 |  否  |
|  10  | 青绿 | 硬挺 | 清脆 | 清晰 | 平坦 | 软粘 |  否  |
|  11  | 浅白 | 硬挺 | 清脆 | 模糊 | 平坦 | 硬滑 |  否  |
|  12  | 浅白 | 蜷缩 | 浊响 | 模糊 | 平坦 | 软粘 |  否  |
|  13  | 青绿 | 稍蜷 | 浊响 | 稍糊 | 凹陷 | 硬滑 |  否  |
|  14  | 浅白 | 稍蜷 | 沉闷 | 稍糊 | 凹陷 | 硬滑 |  否  |
|  15  | 乌黑 | 稍蜷 | 浊响 | 清晰 | 稍凹 | 软粘 |  否  |
|  16  | 浅白 | 蜷缩 | 浊响 | 模糊 | 平坦 | 硬滑 |  否  |
|  17  | 青绿 | 蜷缩 | 沉闷 | 稍糊 | 稍凹 | 硬滑 |  否  |

生成的决策树：
![决策树举例](https://img-blog.csdn.net/20180724162914545?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FtMjkwMzMzNTY2/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)


物理学上，**熵**(Entropy)是“混乱”程度的量度。系统越有序，熵值越低；系统越混乱或者分散，熵值越高;

信息理论： 
- 当系统的有序状态一致时，数据越集中的地方熵值越小，数据越分散的地方熵值越大。这是从信息的完整性上进行的描述。
- 当数据量一致时，系统越有序，熵值越低；系统越混乱或者分散，熵值越高。这是从信息的有序性上进行的描述。

**信息熵**（information entropy）是用来衡量一个随机变量出现的期望值。**如果信息的不确定性越大，熵的值也就越大，出现的各种情况也就越多**。

假如事件D的分类划分是`$(D_1,D_2,…,D_n)$`，每部分发生的概率是`$(p_1,p_2,…,p_n)$`，那信息熵定义为公式如下： 
```math
Entropy(D)=−\sum_{k=1}^np_k \log_2 p_k
```
设样本集`$D$`按离散属性`$a$`有`$V$`个不同的取值`$\{a^1,a^2,...,a^V\}$`，若使用属性`$a$`来对样本集`$D$`进行划分，则会产生`$V$`个分支节点，其中第`$v$`个分支节点包含了`$D$`中所有在属性`$a$`上的取值为`$a^v$`的样本，记为`$D^v$`。

**信息增益**（information gain）是指信息划分前后的熵的变化，也就是说，信息增益就是原有信息熵与属性划分后信息熵（需要对划分后的信息熵取期望值）的差值，具体计算如下：
```math
Gain(D,a)=Entropy(D)−\sum_{v=1}^v{{|D^v|}\over{|D|}}Entropy(D^v)
```
**信息增益越大，则意味着使用属性`$a$`来划分所获得的“纯度提升”越大**。可以利用信息增益来进行决策树的划分属性选择，属性选择方法定义为：
```math
a_*={\arg\max}_{\alpha \in A}Gain(D,a)
```
使用信息增益来划分训练数据集的属性，存在偏向于选择取值较多的属性的问题，使用信息增益率可以对这一问题进行纠正。

**信息增益率**(information gain ratio)：是用信息增益`$Gain(D,a)$`和训练数据集`$D$`关于属性`$a$`的值的熵`$SplitInformation(D,a)$`的比值来共同定义的。

采用与信息增益相同的符号表示，则增益率定义为：
```math
Gain_{ratio}(D,a)={{Gain(D,a)}\over{SplitInformation(D,a)}}
```
其中
```math
SplitInformation(D,a)=-\sum_{v=1}^{V}{{D^v}\over{D}}\log_2{{D^v}\over{D}}
```
称为属性`$a$`的“固有值”。

### ID3算法

决策树算法ID3的基本思想：

首先找出最有判别力的属性，把样例分成多个子集，每个子集又选择最有判别力的属性进行划分，一直进行到所有子集仅包含同一类型的数据为止。最后得到一棵决策树。

[J.R.Quinlan, 1986]的工作主要是引进了信息论中的信息增益，他将其称为信息增益（information gain），作为属性判别能力的度量，设计了构造决策树的递归算法。

ID3算法：

1. 对当前例子集合，计算各属性的信息增益；
2. 选择信息增益最大的属性`$A_k$`；
3. 把在`$A_k$`处取值相同的例子归于同一子集，`$A_k$`取几个值就得几个子集；
4. 对既含正例又含反例的子集，递归调用建树算法；
5. 若子集仅含正例或反例，对应分枝标上P或N，返回调用处。

### C4.5算法

C4.5算法[J.R.Quinlan, 1993]是ID3算法的一种改进。

改进
- 用信息增益率来选择属性，克服了用信息增益选择属性偏向选择多值属性的不足
- 在构造树的过程中进行剪枝
- 对连续属性进行离散化
- 能够对不完整的数据进行处理

C4.5算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。


## 剪枝处理

刚开始我提到，决策树可以看作是一系列`$if-then$`规则的集合。这个规则集合有一个重要的性质：互斥并且完备。意思就是说，拿来任意一个实例，顺着规则的起点（根结点）出发，最终都有且只有一条路径到达某一个具体的叶结点（具体的分类），并且不会出现实例无法分类的情况。

最后学习到的决策树对训练数据集能达到令人满意的结果，但是对于未知的测试集来说却未必有很好的分类能力。即数据集的泛化能力不能保证。

为了提高决策树的泛化能力，需要对树进行剪枝(Pruning)，把过于细分的叶结点（通常是数据量过少导致噪声数据的影响增加）去掉而回退到其父结点或更高的结点，使其父结点或更高的结点变为叶结点。

### 预剪枝

预剪枝是指在决策树生成过程中，对每个节点在划分前后进行估计，若当前节点的划分不能带来决策树泛化性能提升，则停止划分并将当前节点标记为叶节点。
![预剪枝算法流程](https://img-blog.csdn.net/20180724163010468?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FtMjkwMzMzNTY2/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

### 后剪枝

后剪枝则是先从训练集生成一颗完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来泛化性能提升，则将该子树替换为叶结点。
![后剪枝算法流程](https://img-blog.csdn.net/20180724163032640?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FtMjkwMzMzNTY2/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

## 分类与回归树（CART）

CART是在给定输入随机变量`$X$`条件下输出随机变量`$Y$`的条件概率分布的学习方法。

CART 决策树既可以用于分类，也可以用于回归；对回归树CART算法用**平方误差最小化准则**来选择特征，对分类树用**基尼指数最小化准则**选择特征。CART由以下两步组成：

1. 决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大；
2. 决策树剪枝：用验证数据集对已生成的书进行剪枝并选择最优子树，这是用损失函数最小作为剪枝的标准。

### CART生成

#### 回归树生成

##### CART 回归树算法推导
- 一个回归树对应着输入空间/**特征空间**的一个**划分**以及在划分单元上的**输出值**；

- 假设已将输入空间划分为`M`个单元：`$\{R_1,..,R_m,..,R_M\}$`，并在每个单元上对应有输出值 `$c_m$`，则该回归树可表示为

```math
f(x)=\sum_{m=1}^Mc_mI(x \in R_m)
```

> `$I(x)$` 为指示函数
- 如果已经划分好了输入空间，通常使用**平方误差**作为损失函数来表示回归树对于训练数据的预测误差，通过最小化损失函数来求解每个划分单元的**最优输出值**。
- 如果使用**平方误差**，易知**最优输出值**即每个划分单元上所有实例的均值

```math
\hat{c_m}=avg(y_i|x_i \in R_m)
```

> 选用**平方误差**作为损失的原因

##### 如何划分输入空间

- 一个启发式方法是：**以特征向量中的某一个特征为标准进行切分**。

  假设选择**特征向量中第 `j` 个变量**作为**切分变量**，然后选择**某个实例中第 `j` 个值 `s`** 作为**切分点**，则定义如下两个划分单元：
```math
R_1(j,s)=\{x|x^{(j)} \leq s\}, R_2(j,s)=\{x|x^{(j)} > s\}
```
> 详情见《统计学习方法》8.4，页码P148。

- 遍历**每个实例**的第`j`个值`s`，选择满足以下条件的作为**最优切分变量`j`和切分点`s`**

```math
{{\min}\atop{j,s}}[{{\min}\atop{c_1}}\sum_{x_i \in R_1}(y_i-c_1)^2+{{\min}\atop{c_2}}\sum_{x_i \in R_2}(y_i-c_2)^2]
```

其中输出值 `c1` 和 `c2` 分别为:

```math
\hat{c_1}=avg(y_i|x_i \in R_1(j,s)), \hat{c_2}=avg(y_i|x_i \in R_2(j,s))
```

- 接着，继续对两个子空间重复以上步骤，直到满足条件为止；得到将输入空间划分为`M`个区域的决策树

```math
f(x)=\sum_{m=1}^M\hat{c_m}I(x \in R_m)
```

##### 示例: 选择切分变量与切分点
> 《统计学习方法》 8.4.2 例8.4 页码P149
- 训练集

 `$x_i$` | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10
-----|---|---|---|---|---|---|---|---|---|---
 `$y_i$` |5.56|5.70|5.91|6.40|6.80|7.05|8.90|8.70|9.00|9.05
- 这里只有一个特征，即`j=1`；然后遍历每个实例的值作为**切分点**

  `s = {1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5}`
- 以 `s=1.5` 为例

```math
R_1(1,1.5)=\{1\}

R_2(1,1.5)=\{2,3,4,5,6,7,8,9,10\}

c_1=\frac{1}{|R_1|}\sum_{x_i \in R_1}y_i=\frac{1}{1}\sum_{x_i \in R_1}y_i=5.56

c_2=\frac{1}{|R_2|}\sum_{x_i \in R_2}y_i=\frac{1}{9}\sum_{x_i \in R_2}y_i=7.50

m(s)={{\min}\atop{c_1}}\sum_{x_i \in R_1}(y_i-c_1)^2+{{\min}\atop{c_2}}\sum_{x_i \in R_2}(y_i-c_2)^2=0+15.72=15.72
```

  所有 `m(s)` 的计算结果如下

  `$s$` | 1.5 | 2.5 | 3.5 | 4.5 | 5.5 | 6.5 | 7.5 | 8.5 | 9.5 
  ----|---|---|---|---|---|---|---|---|---
  `$m(s)$`|15.72|12.07|8.36|5.78|3.91|1.93|8.01|11.73|15.74

- 当 `s=6.5` 时 `m(s)` 达到最小值，此时

```math
R_1(1,6.5)=\{1,2,3,4,5,6\}

R_2(1,6.5)=\{7,8,9,10\}

c_1=\frac{1}{|R_1|}\sum_{x_i \in R_1}y_i=\frac{1}{6}\sum_{x_i \in R_1}y_i=6.24

c_2=\frac{1}{|R_2|}\sum_{x_i \in R_2}y_i=\frac{1}{4}\sum_{x_i \in R_2}y_i=8.91

```

- 所以第一棵回归树`$T_1(x)$`为

```math
T_1(x)=\{{{6.24, x<6.5} \atop {8.91, x \geq 6.5}}

f_1(x)=T_1(x)
```

#### 分类树的生成

> 分类树用**基尼指数**(Gini index)来选择最优特征，同时决定该特征的最优二值切分点。

分类问题中，假设有`$K$`个类，样本点属于第`$k$`类的概率为`$p$`，则概率分布的基尼指数定义为：
```math
Gini(p)=\sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^np_k^2
```
对于二分类问题，若样本点属于第1类的概率为`$p$`，则概率分布的基尼指数为:
```math
Gini(p)=\sum_{k=1}^Kp_k(1-p_k)=p(1-p)+(1-p)p=2p(1-p)
```
对于给定的样本集合`$D$`，其基尼指数为:
```math
Gini(D)=1-\sum_{k=1}^K({{|C_k|}\over{|D|}})^2
```
这里，`$C_k$`是`$D$`中属于类`$k$`的样本子集，`$K$`是类的个数。

如果样本集合`$D$`根据特征`$A$`是否取某一可能指`$a$`被分割成`$D_1$`和`$D_2$`两部分，即
```math
D_1=\{(x,y) \in D|A(x)=a\}, D_2=D-D_1
```
则在特征`$A$`的条件下，集合`$D$`的基尼指数定义为：
```math
Gini(D,A)={{|D_1|}\over{|D|}}Gini(D_1)+{{|D_2|}\over{|D|}}Gini(D_2)
```
> 基尼指数`$Gini(D)$`表示集合`$D$`的不确定性，基尼指数`$Gini(D,A)$`表示经特征`$A=a$`分割后集合`$D$`的不确定性。基尼指数越大，样本集合的不确定性也就越大。

### CART剪枝

CART剪枝算法由两步组成：
1. 首先从生成算法产生的决策树`$T_0$`底端开始不断剪枝，直到`$T_0$`的根节点，形成一个子树序列`$\{T_0,T_1,...,T_n\}$`；
2. 然后通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树。

[详情见后剪枝算法步骤](http://note.youdao.com/noteshare?id=5560b18c6b422232208accdbea191fb8&sub=1165631AC6CF418EAA5A1B5BDE45D4EF)。