## 神经网络

- NLP-机器学习笔试面试题解析 [Github链接](https://github.com/WerterHong/Machine-Learning-Algorithm-NLP/)
- **神经网络** (若公式显示错误，请点击此链接) [有道云笔记](http://note.youdao.com/noteshare?id=94df62743e6e52fa440f55a7c9cc24d7&sub=3ACC77B708234F399B30F04031655EE2)

### 1. 卷积神经网络[`Convolutional Neural Network`](http://cs231n.github.io/convolutional-networks/)

卷积神经网络`CNN`大致由卷积`Convolution`层、池化`Pooling`层、全连接`Fully connecnted`层三个模块组成，全连接层就是一个`BP`神经网络

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/94df62743e6e52fa440f55a7c9cc24d7/ACF671B7688346ECA82710C98B0F935D?ynotemdtimestamp=1565698102285" height=250 />
    <br/>
    <strong>Fig</strong>. 卷积神经网络
</p>

#### 1.1 卷积层`Convolutional layer`

卷积层对隐含单元和输入单元间的连接加以限制：**每个隐含单元仅仅只能连接输入单元的一部分**。

卷积运算就是将原始图片的与特定的`Feature Detector(filter)`做卷积运算(符号`⊗`)，下图为例（`5*5`的图像与`3*3`的卷积核相乘后再相加得到`3*3`的`feature map`）：

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/94df62743e6e52fa440f55a7c9cc24d7/A7B2887D1E794C0CA4ED0D4B680B354B?ynotemdtimestamp=1565698671007" height=250 />
    <br/>
    <strong>Fig</strong>. 卷积核
</p>

卷积运算为：

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/94df62743e6e52fa440f55a7c9cc24d7/1668EE024B4F4B049F2D0AF5D031E325?ynotemdtimestamp=1565698845865" height=300 />
    <br/>
    <strong>Fig</strong>. 卷积运算
</p>

卷积层一个输出单元的大小有以下三个量控制：`depth`, `stride` 和 `zero-padding`。

- ==深度==(`depth`) : 控制输出单元的深度，也就是**卷积滤波器`filter`的个数**，连接同一块区域的神经元个数。又名：`depth column`
- ==步幅==(`stride`)：是**每次卷积滤波器移动的步长**。步幅大小通常为`1`，意味着滤镜逐个像素地滑动。通过增加步幅大小，滤波器在输入上滑动的间隔更大，因此单元之间的重叠更少。
- ==补零==(`zero-padding`)：通过在**输入单元周围补零**来改变输入单元整体大小，从而控制输出单元的空间大小。

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/94df62743e6e52fa440f55a7c9cc24d7/0124109F265548A488AADF3B420435C5?ynotemdtimestamp=1565698311879" height =450 />
    <br/>
    <strong>Fig</strong>. 深度为 2 | 步幅为 1 | 补零为 1
</p>

#### 1.2 池化层`Pooling layer`

==池化==（`pooling`）又叫**下采样**（`downsample`），功能是不断**降低维数**，以减少网络中的参数和计算次数。这**缩短了训练时间**并**控制过度拟合**。

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/94df62743e6e52fa440f55a7c9cc24d7/0448C6A592B34D9CA14DDB830558E9F5?ynotemdtimestamp=1565698311879" height=200 />
    <br/>
    <strong>Fig</strong>. 2 * 2 池化层 | 步幅为 2
</p>

常见池化层运算：
- 最大池化（`Max Pooling`）：取`4`个点的最大值
- 均值池化（`Mean Pooling`）:取`4`个点的均值
- 高斯池化：借鉴高斯模糊的方法
- 可训练池化：训练函数`f` ，接受`4`个点为输入，输出`1`个点。

> 池化操作将保存**深度大小不变**。如果池化层的输入单元大小不是二的整数倍，一般采取边缘补零（`zero-padding`）的方式补成`2`的倍数，然后再池化。

#### 1.3 全连接层`Fully-connected layer`

==全连接层==：经过若干层的卷积, 池化操作后, 将得到的**特征图依次按行展开**（`flatten`）, 连接成向量, 输入全连接网络。

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/94df62743e6e52fa440f55a7c9cc24d7/7608EC7F5C934D4C9B40BCBC356823E5?ynotemdtimestamp=1565698311879" height=350 />
    <br/>
    <strong>Fig</strong>. 全连接层
</p>

> 全连接层和卷积层互换，见 [Convolutional-Networks](http://cs231n.github.io/convolutional-networks/)


### 2. 递归神经网络`Recurrent  Neural Network`


### 3. `RNN`变体

#### 3.1 `LSTM`

#### 3.2 `GRU`