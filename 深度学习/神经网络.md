## 神经网络

- NLP-机器学习笔试面试题解析 [Github链接](https://github.com/WerterHong/Machine-Learning-Algorithm-NLP/)
- **神经网络** (若公式显示错误，请点击此链接) [有道云笔记](http://note.youdao.com/noteshare?id=80494c1471fc041b4ba54bd24eccbdcc&sub=215E424205A34D57A8ABFD274246ACD8)

### 1. 卷积神经网络[`Convolutional Neural Networks`](http://cs231n.github.io/convolutional-networks/)

卷积神经网络`CNN`大致由卷积`Convolution`层、池化`Pooling`层、全连接`Fully connecnted`层三个模块组成，全连接层就是一个`BP`神经网络

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/94df62743e6e52fa440f55a7c9cc24d7/ACF671B7688346ECA82710C98B0F935D?ynotemdtimestamp=1565698102285" height=250 />
    <br/>
    <strong>Fig</strong>. 卷积神经网络
</p>

#### 1.1 卷积层`Convolutional layer`

卷积层对隐含单元和输入单元间的连接加以限制：**每个隐含单元仅仅只能连接输入单元的一部分**。

卷积运算就是将原始图片的与特定的`Feature Detector(filter)`做卷积运算(符号`⊗`)，下图为例（`5*5`的图像与`3*3`的卷积核相乘后再相加得到`3*3`的`feature map`）：

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/94df62743e6e52fa440f55a7c9cc24d7/A7B2887D1E794C0CA4ED0D4B680B354B?ynotemdtimestamp=1565698671007" height=250 />
    <br/>
    <strong>Fig</strong>. 卷积核
</p>

卷积运算为：

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/94df62743e6e52fa440f55a7c9cc24d7/1668EE024B4F4B049F2D0AF5D031E325?ynotemdtimestamp=1565698845865" height=300 />
    <br/>
    <strong>Fig</strong>. 卷积运算
</p>

卷积层一个输出单元的大小有以下三个量控制：`depth`, `stride` 和 `zero-padding`。

- ==深度==(`depth`) : 控制输出单元的深度，也就是**卷积滤波器`filter`的个数**，连接同一块区域的神经元个数。又名：`depth column`
- ==步幅==(`stride`)：是**每次卷积滤波器移动的步长**。步幅大小通常为`1`，意味着滤镜逐个像素地滑动。通过增加步幅大小，滤波器在输入上滑动的间隔更大，因此单元之间的重叠更少。
- ==补零==(`zero-padding`)：通过在**输入单元周围补零**来改变输入单元整体大小，从而控制输出单元的空间大小。

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/94df62743e6e52fa440f55a7c9cc24d7/62E2968206DD4F8998550728A6BC13D4?ynotemdtimestamp=1565780229989" height =450 />
    <br/>
    <strong>Fig</strong>. 深度为 2 | 步幅为 1 | 补零为 1
</p>

#### 1.2 池化层`Pooling layer`

==池化==（`pooling`）又叫**下采样**（`downsample`），功能是不断**降低维数**，以减少网络中的参数和计算次数。这**缩短了训练时间**并**控制过度拟合**。

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/94df62743e6e52fa440f55a7c9cc24d7/0448C6A592B34D9CA14DDB830558E9F5?ynotemdtimestamp=1565698311879" height=200 />
    <br/>
    <strong>Fig</strong>. 2 * 2 池化层 | 步幅为 2
</p>

常见池化层运算：
- 最大池化（`Max Pooling`）：取`4`个点的最大值
- 均值池化（`Mean Pooling`）:取`4`个点的均值
- 高斯池化：借鉴高斯模糊的方法
- 可训练池化：训练函数`ff` ，接受`4`个点为输入，输出`1`个点。

> 池化操作将保存**深度大小不变**。如果池化层的输入单元大小不是二的整数倍，一般采取边缘补零（`zero-padding`）的方式补成`2`的倍数，然后再池化。

#### 1.3 全连接层`Fully-connected layer`

==全连接层==：经过若干层的卷积, 池化操作后, 将得到的**特征图依次按行展开**（`flatten`）, 连接成向量, 输入全连接网络。

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/94df62743e6e52fa440f55a7c9cc24d7/7608EC7F5C934D4C9B40BCBC356823E5?ynotemdtimestamp=1565698311879" height=350 />
    <br/>
    <strong>Fig</strong>. 全连接层
</p>

> 全连接层和卷积层互换，见 [Convolutional-Networks](http://cs231n.github.io/convolutional-networks/)

> `CNN`是通过**梯度下降**和**反向传播算法**进行训练的, 全连接层的梯度公式与[`BP`网络](http://note.youdao.com/noteshare?id=a2b90803e7396be4bd69bda62ba77562&sub=0EC7B5D646CA4C83B09AB6553D4B8B77)完全一样。

#### 1.4 `CNN`中的局部连接和权值共享

卷积层最主要的两个特征就是**局部连接**和**权值共享**，也叫做稀疏连接和参数共享。

==局部连接==，就是**卷积层的节点仅仅和其前一层的部分节点相连接**，只用来**学习局部特征**。

==权值共享==，比如一个`3*3`的卷积核，共`9`个参数，它会和输入图片的不同区域作卷积，来检测相同的特征。而只有**不同的卷积核才会对应不同的权值参数**，来检测不同的特征。

> ==前馈神经网络的缺点==：
> - 每次网络的输出只依赖当前的输入，没有考虑不同时刻输入的相互影响
> - 输入和输出的维度都是固定的，没有考虑到序列结构数据长度的不固定性

### 2. 循环神经网络`Recurrent  Neural Networks`

- 循环神经网络是一类专门用于**处理时序数据样本**的神经网络，它的每一层不仅输出给下一层，同时还输出一个**隐状态**，给当前层在处理下一个样本时使用。
- 循环神经网络可以**处理序列长度不同的数据**，它可以看作是带自循环反馈的**全连接神经网络**。

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/94df62743e6e52fa440f55a7c9cc24d7/6183A6B57F4B4C8589CFF540F66C908B?ynotemdtimestamp=1565779788055" height=350 />
    <br/>
    <strong>Fig</strong>. 循环神经网络 RNN
</p>

其中，
- `$x^{(t)}$`是序列索引`t`时刻的输入。例如，`$x^{(1)}$`是第二个词的`one-hot`编码向量
- `$h^{(t)}$`是序列索引`t`时模型的隐藏状态。`$h^{(t)}$`由`$x^{(t)}$`和`$h^{(t-1)}$`共同决定
- `$o^{(t)}$`是序列索引`t`时模型的输出。`$o^{(t)}$`只由模型当前的隐藏状态`$h^{(t)}$`决定
- `$L^{(t)}$`是序列索引`t`时模型的损失函数
- `$y^{(t)}$`是序列索引`t`时训练样本序列的真实输出
- `U,W,V`这三个矩阵是我们的模型的线性关系参数，它在整个RNN网络中是共享的，体现了RNN的模型的“循环反馈”的思想

> `RNN` ==前向传播算法==：

```math
h^{(t)}=\sigma\left(z^{(t)}\right)=\sigma\left(U x^{(t)}+W h^{(t-1)}+b\right)

\hat{y}^{(t)}=\sigma\left(o^{(t)}\right)=\sigma(V h^{(t)}+c)
```

> 通过损失函数（**交叉熵损失函数**）`$L^{(t)}$`，比如对数似然损失函数，我们可以量化模型在当前位置的损失，即`$\hat{y}^{(t)}$`和`$y^{(t)}$`的差距。

> `RNN` ==反向传播算法==：
>
> 通过**梯度下降法**一轮轮的迭代，得到合适的`RNN`模型参数`U,W,V,b,c`，这里所有的`U,W,V,b,c`在序列的各个位置是共享的，反向传播时我们更新的是相同的参数。

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/94df62743e6e52fa440f55a7c9cc24d7/F662022680514E87AE3A367825BBE03A?ynotemdtimestamp=1565779788055" />
    <br/>
    <strong>Fig</strong>. RNN 短期记忆
</p>

==梯度消失==与==梯度爆炸==：

`RNN`反向传播算法中使用梯度下降更新参数`$W: W-\eta \frac{\partial L}{\partial W}$`，对时刻`t=3`求偏导：

```math
\frac{\partial L^{(3)}}{\partial V}=\frac{\partial L^{(3)}}{\partial o^{(3)}} \frac{\partial o^{(3)}}{\partial V}

{\frac{\partial L^{(3)}}{\partial U}=\frac{\partial L^{(3)}}{\partial o^{(3)}} \frac{\partial o^{(3)}}{\partial h^{(3)}} \frac{\partial h^{(3)}}{\partial U}+\frac{\partial L^{(3)}}{\partial o^{(3)}} \frac{\partial o^{(3)}}{\partial h^{(3)}} \frac{\partial h^{(3)}}{\partial h^{(2)}} \frac{\partial h^{(2)}}{\partial U}+\frac{\partial L^{(3)}}{\partial o^{(3)}} \frac{\partial o^{(3)}}{\partial h^{(3)}} \frac{\partial h^{(3)}}{\partial h^{(2)}} \frac{\partial h^{(2)}}{\partial h^{(1)}} \frac{\partial h^{(1)}}{\partial U}}

{\frac{\partial L^{(3)}}{\partial W}=\frac{\partial L^{(3)}}{\partial o^{(3)}} \frac{\partial o^{(3)}}{\partial h^{(3)}} \frac{\partial h^{(3)}}{\partial W}+\frac{\partial L^{(3)}}{\partial o^{(3)}} \frac{\partial o^{(3)}}{\partial h^{(3)}} \frac{\partial h^{(3)}}{\partial h^{(2)}} \frac{\partial h^{(2)}}{\partial W}+\frac{\partial L^{(3)}}{\partial h^{(3)}} \frac{\partial o^{(3)}}{\partial h^{(3)}} \frac{\partial h^{(3)}}{\partial h^{(2)}} \frac{\partial h^{(2)}}{\partial h^{(1)}} \frac{\partial h^{(1)}}{\partial W}}
```
可以看出对于`V`求偏导并没有长期依赖，但是对于`W,U`求偏导，会随着时间序列产生长期依赖。因为`$h^{(t)}$`随着时间序列向前传播，而`$h^{(t)}$`又是`W,U`的函数。

```math
h^{(t)}=\sigma\left(z^{(t)}\right)=\sigma\left(U x^{(t)}+W h^{(t-1)}+b\right)
```
于是任意时刻对`W,U`求偏导的公式：

```math
\frac{\partial L^{(t)}}{\partial W}=\sum_{k=0}^{t} \frac{\partial L^{(t)}}{\partial o^{(t)}} \frac{\partial o^{(t)}}{\partial h^{(t)}}\left(\prod_{j=k+1}^{t} \frac{\partial h^{(j)}}{\partial h^{(j-1)}}\right) \frac{\partial h^{(k)}}{\partial W}
```
若激活函数为`sigmoid`，其导数图像如下所示：

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/94df62743e6e52fa440f55a7c9cc24d7/0F82846443C74F93986BE23DF4D5E33F?ynotemdtimestamp=1565779788055" height=300 />
    <br/>
    <strong>Fig</strong>. Sigmoid 函数的导数
</p>

由图知`$Sigmoid^{\prime} \in [0, 0.25]$`，则，

```math
\prod_{j=k+1}^{t} \frac{\partial h_{j}}{\partial h_{j-1}}=\prod_{j=k+1}^{t} \operatorname{sigmoid} ^{\prime} W
```
当序列`t`很大时，

1. 如果参数`W,U`大于`0`小于`1`，上式就会**趋近于`0`**，此时会产生梯度消失
2. 如果参数`W,U`大于`1`，上式就会**趋近于无穷**，此时会产生梯度爆炸

> 梯度消失和梯度爆炸==产生原因与解决方法==：通过反向传播算法更新梯度的公式可以看到，影响梯度更新的有，初始权重、激活函数、梯度流动方式、损失值过大等
>
> (1) 初始权重带来的影响：神经网络权重初始化不当
> - 使用`Xavier`初始化法或者`MSRA`初始化法，使得在深度网络的每一层，激活值都有很好的分布。
> - 使用预训练模型，初始化已有网络层的权重。
> - 使用权重正则化
>
> (2) 激活函数带来的影响：激活函数选择不当
> - 使用`Relu`，需要小心地调节学习速率
> - 考虑`Relu`的变种，如`Leaky Relu`
> - 一般不使用`sigmoid`
>
> (3) 梯度流动方式带来的影响：网络结构本身的问题，如RNN
> - 设置梯度剪切阈值（`gradient clipping threshold`），一旦梯度超过该值，直接设置为该值
> - 使用沿时间的截断反向传导方法
> - 使用更加复杂的`RNN`结构，例如`LSTM`
>
> (4) 损失值过大带来的影响：数据集的问题，如标注不准等。
>
> 还可以利用`batch norm`和**残差结构**来解决梯度消失和梯度爆炸问题。

### 3. `Long Short-Term Memory, LSTM`

`RNN`模型具有神经网络模块链式结构:

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/94df62743e6e52fa440f55a7c9cc24d7/E1D8EA8D0DF04E908EF4CCDE19A7257D?ynotemdtimestamp=1565779788055" width=500 />
    <br/>
    <strong>Fig</strong>. RNN 模型
</p>

> `RNN`的缺陷：
>
> 1. `RNN` 有**短期记忆**问题（`05`时刻橙色区域相比占很大部分），无法处理很长的输入序列
> 2. 训练 `RNN` 需要投入**极大的成本**
> 3. `RNN` 有**梯度消失**问题
>
> `LSTM`能尽量==避免梯度爆炸或者梯度消失的原因==有：
>
> 1. 这里的遗忘门是**矩阵元素相乘**，而不是矩阵相乘
> 2. 矩阵元素相乘，可能会在不同的时间点乘以一个不同的遗忘门
> 3. 遗忘门是一个`sigmoid`函数，所以矩阵元素相乘的结果，会保证在`(0,1)`之间
> 4. 从最后的隐藏单元状态，反向传播到第一个单元状态，在反向传播的路径上，我们只通过**一个单一的非线性`tanh`向后传播**，而不是在每一个时间步长中单独设置`tanh`函数

#### 3.1 `LSTM` 模型结构

`LSTM`结构如下图：

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/94df62743e6e52fa440f55a7c9cc24d7/18DCED5025614124B014B7BAAF1BD370?ynotemdtimestamp=1565779788055" width=600 />
    <br/>
    <strong>Fig</strong>. LSTM 模型
</p>

##### 3.1.1 遗忘门

遗忘门（`forget gate`）是控制是否遗忘的，在`LSTM`中即以一定**的概率控制是否遗忘上一层的隐藏细胞状态**。遗忘门子结构如下图所示：

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/94df62743e6e52fa440f55a7c9cc24d7/ED259B6A5B0F4DBE9289D00E2B5772FD?ynotemdtimestamp=1565779788055" width=600 />
    <br/>
    <strong>Fig</strong>. 遗忘门
</p>

> 可以看到这里的`$f_t$`由输入的`$x_t$`和`$h_{t−1}$`得到，用来控制`$C_{t−1}$`中的**信息的遗忘程度**。`$f_t$`中的每个值都是`0-1`中的一个数，下界`0`代表完全遗忘，上界`1`代表完全不变。

##### 3.1.2 输入门

输入门（`input gate`）负责**处理当前序列位置的输入**，它的子结构如下图：

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/94df62743e6e52fa440f55a7c9cc24d7/FCFF51D28E554F308BB234177F09B8B2?ynotemdtimestamp=1565780549773" width=600 />
    <br/>
    <strong>Fig</strong>. 输入门
</p>

> 由输入的`$x_t$`和`$h_{t−1}$`得到当前的`$i_t$`用以控制**新状态信息的更新程度**。这里新状态信息`$\tilde{C}$`也是通过输入的`$x_t$`和`$h_{t−1}$`计算得出。

##### 3.1.2 细胞状态更新

**更新`LSTM`细胞状态**。前面的遗忘门和输入门的结果都会作用于细胞状态。细胞状态更新结构如下图：

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/94df62743e6e52fa440f55a7c9cc24d7/CBBE05324BA14EC1994417F18AB1EE88?ynotemdtimestamp=1565780549773" width=600 />
    <br/>
    <strong>Fig</strong>. 更新细胞状态
</p>

> `$C_t$`就可以通过上式计算得出，通俗的说就是遗忘一些旧信息，更新一些新信息进去。

##### 3.1.2 输出门

有了新的隐藏细胞状态，我们就可以来看输出门（`output gate`）了，子结构如下：

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/94df62743e6e52fa440f55a7c9cc24d7/6154124CB13F442ABA4C65E75AEDC996?ynotemdtimestamp=1565780549773" width=600 />
    <br/>
    <strong>Fig</strong>. 输出门
</p>

> 根据`$x_t$`和`$h_{t−1}$`得出`$o_t$`用以**控制哪些信息需要作为输出**。

> 概括：
> - 状态信息`$C_t$`的依赖于遗忘门`$f_t$`和输入门`$i_t$`
> - 遗忘门`$f_t$`和输入门`$i_t$`依赖于输入参数中的`$h_{t−1}$`
> - 而当前隐层输出`$h_t$`依赖于`$C_t$`

#### 3.2 `LSTM` 变种

##### `GRU`

门控循环单元`GRU`比较流行的`LSTM`变种，GRU模型中只有两个门：分别是更新门和重置门。具体结构如下图所示：

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/94df62743e6e52fa440f55a7c9cc24d7/F8078C19C9EA4677A8A3FA4D0603B948?ynotemdtimestamp=1565779788055" width=600 />
    <br/>
    <strong>Fig</strong>. GRU 模型
</p>


> 图中的`$z_t$`和`$r_t$`分别表示更新门和重置门。更新门用于控制**前一时刻的状态信息被带入到当前状态中的程度**，更新门的值越大说明前一时刻的状态信息带入越多。重置门控制**前一状态有多少信息被写入到当前的候选集**`$\tilde{h}_t$`上，重置门越小，前一状态的信息被写入的越少。

##### `LSTM + peephole connections`

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/94df62743e6e52fa440f55a7c9cc24d7/179D47D4CB334124B8B7D19321FC2F6A?ynotemdtimestamp=1565779788055" width=600 />
    <br/>
    <strong>Fig</strong>. LSTM + peephole connections
</p>

> 在所有的门之前都与状态线相连，使得状态信息对门的输出值产生影响。但一些论文里只是在部门门前加上这样的连接，而不是所有的门。

##### 耦合遗忘门和输入门

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/94df62743e6e52fa440f55a7c9cc24d7/7A2E8B6CA23E4F73934DA6B8906152FF?ynotemdtimestamp=1565779788055" width=600 />
    <br/>
    <strong>Fig</strong>. 耦合遗忘门和输入门
</p>

> 这一种变体是将遗忘门和输入门耦合在一起，简单来说就是遗忘多少就更新多少新状态，没有遗忘就不更新状态，全部遗忘那就新状态全部更新进去。
