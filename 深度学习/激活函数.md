## 激活函数

- NLP-机器学习笔试面试题解析 [Github链接](https://github.com/WerterHong/Machine-Learning-Algorithm-NLP/)
- **激活函数** (若公式显示错误，请点击此链接) [有道云笔记](http://note.youdao.com/noteshare?id=7231cb5b2cc96c657bb5ad0d17b47b00&sub=FC472BF78D0C473CBA4AEE1EB824F95D)

### 1. 激活函数

加入==非线性激励函数==后，神经网络就有可能学习到**平滑的曲线来分割平面**，而不是用复杂的线性组合逼近平滑曲线来分割平面，使神经网络的**表示能力更强**了，能够**更好的拟合目标函数**。

### 2. 线性激活`Linear activation`函数

恒等（`Identity`）函数或线性激活（`Linear activation`）函数是最简单的激活函数。输出和输入成比例，值域`(-∞,+∞)`。

```math
g(x)=x
```
其函数图像如下：

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/61e22de3ed9ed9bb6d733fedf7245dbc/624A5C5FD28843D5B6A9AB366F69A5FD?ynotemdtimestamp=1565790366212" />
    <br/>
    <strong>Fig</strong>. 线性激活函数
</p>

> 线性激活（`Linear activation`）函数的导数是常数，梯度也是常数，**梯度下降无法工作**。

### 3. 阶跃`Heaviside step`函数

阶跃`Heaviside step`函数通常只在**单层感知器**上有用，单层感知器是神经网络的早期形式，可用于分类线性可分的数据，值域`0`或`1`。

```math
g(x)=\left\{\begin{array}{ll}{1,} & {\text { if } x>0} \\ {0,} & {\text { if } x<0}\end{array}\right.
```

其函数图像如下图：

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/61e22de3ed9ed9bb6d733fedf7245dbc/E8E7F8007B4D4AACB3901AB908D75FEA?ynotemdtimestamp=1565790366212" width=500 />
    <br/>
    <strong>Fig</strong>. 阶跃函数
</p>

> 阶跃`Heaviside step`函数存在与线性激活函数一样的缺点。

### 4. `Sigmoid`函数

`Sigmoid`函数是神经网络中常用的激活函数之一，`logistic`函数也就是经常说的`sigmoid`函数，它的几何形状也就是一条`sigmoid`曲线（S型曲线），其定义为：

```math
\sigma ( x ) = \frac { 1 } { 1 + e ^ { - x } }
```

该函数的定义域为`(-∞,+∞)`，值域`(0,1)`，其函数图像如下：

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/61e22de3ed9ed9bb6d733fedf7245dbc/F3209911118E4203B248F23986530BA8?ynotemdtimestamp=1565790366212" width=600 />
    <br/>
    <strong>Fig</strong>. Sigmoid 函数与其导数
</p>

`Sigmoid`函数具有如下的特性：当`x`趋近于负无穷时，`y`趋近于`0`；当`x`趋近于正无穷时，`y`趋近于`1`；当`x= 0`时，`y=0.5`。

> `sigmoid`函数的导函数具有以下形式：

```math
\sigma ^ { \prime } ( x ) = \sigma ( x ) [ 1 - \sigma ( x ) ]
```

> 函数`log σ(x)`和`log (1-σ(x))`的导函数分别为：

```math
[ \log \sigma ( x ) ] ^ { \prime } = 1 - \sigma ( x ) , \quad [ \log ( 1 - \sigma ( x ) ) ] ^ { \prime } = - \sigma ( x )
```

> `Sigmoid`函数的==优缺点==：
>
> **优点**：
>
> 1. `Sigmoid`函数的输出映射在`(0,1)`之间，**单调连续，输出范围有限，优化稳定，可以用作输出层**
> 2. **求导容易**
> 3. 便于前向传输
>
> **缺点**：
>
> 1. 由于其软饱和性，容易产生**梯度消失**，导致训练出现问题
> 2. 其输出并**不是以`0`为中心**的（`zero-centered`），`sigmoid`输出均值为`0.5`
> 3. **幂运算求解耗时**

> sigmoid函数可用在**网络最后一层**，作为输出层进行二分类，尽量不要使用在隐藏层。

### 5. `tanh`函数

`tanh`全称`Hyperbolic Tangent`，由基本双曲函数双曲正弦和双曲余弦推导而来。

```math
\tanh (x)=\frac{\sinh x}{\cosh x}=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}=2*\operatorname{sigmoid} (2*x)-1
```

`tanh`函数的定义域为`(-∞,+∞)`，值域`(-1,1)`，其函数图像如下：

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/61e22de3ed9ed9bb6d733fedf7245dbc/5D222A1DFCB146839725EF2C0DEAD3E0?ynotemdtimestamp=1565790366212" width=600 />
    <br/>
    <strong>Fig</strong>. tanh 函数与其导数
</p>

> `tanh`函数的导函数为：

```math
\tanh^{\prime}=1-\tanh^2
```

> `tanh`函数的==优缺点==：
>
> 优点：
> `tanh`函数的**收敛速度要比`Sigmoid`快**。因为`tanh`的输出均值为`0`，`SGD`会更接近 `natural gradient`，从而降低所需的迭代次数
>
>缺点：
> 1. `tanh`函数与`Sigmoid`函数一样也具有软饱和性，没有改变`Sigmoid`函数的最大问题——由于饱和性产生的**梯度消失**
> 2. **幂运算求解耗时**，对于规模比较大的深度网络，这会较大地增加训练时间

### 6. `ReLU`函数

`ReLU`函数又称为修正线性单元（`Rectified Linear Unit`），是一种分段线性函数，其弥补了`sigmoid`函数以及`tanh`函数的梯度消失问题，值域为`[0,+∞)`。`ReLU`函数定义如下：

```math
g(x)=\left\{\begin{array}{ll}{x,} & {\text { if } x>0} \\ {0,} & {\text { if } x \leq 0}\end{array}\right.
```
`ReLU`函数如下图：

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/61e22de3ed9ed9bb6d733fedf7245dbc/C078E487673248C2993A6356C5D422C8?ynotemdtimestamp=1565790366212" />
    <br/>
    <strong>Fig</strong>. Relu 函数与其导数
</p>

`ReLU`函数的导函数为：

```math
g^{\prime}(x)=\left\{\begin{array}{ll}{1,} & {\text { if } x>0} \\ {0,} & {\text { if } x \leq 0}\end{array}\right.
```

> `ReLU`函数的==优缺点==：
>
> 优点：
> 1. 在输入为正数的时候（对于大多数输入空间来说），**不存在梯度消失问题**
> 2. **计算速度要快很多**。`ReLU`函数只有线性关系，不管是前向传播还是反向传播，都比`sigmod`和`tanh`要快很多。（`sigmod`和`tanh`存在幂运算，计算速度会比较慢）
> 3. 收敛速度远快于`sigmoid`和`tanh`
>
> 缺点：
> 1. 当输入为负时，梯度为`0`，会产生梯度消失问题
> 2. 其输出并**不是以`0`为中心**的（`zero-centered`）
> 3. `Dead ReLU Problem`，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新（`learning rate`太高导致在训练过程中参数更新太大：`Xavier`初始化方法）

### 7. `PReLU`函数

`PReLU`全称`Parametric ReLU`函数：为了解决`ReLU`函数中的`Dead ReLU Problem`，提出了将`ReLU`的前半段设为`αx`而非`0`，`α`取值在`(0,1)`之间。函数（值域为`(-∞, +∞)`）定义为：

```math
g(x)=\max(\alpha x,x)=\left\{\begin{array}{rll}{\alpha x} & {\text { for }} & {x<0} \\ {x} & {\text { for }} & {x \geq 0}\end{array}\right.
```
`PReLU`函数的导函数为：

```math
g^{\prime}(x)=\left\{\begin{array}{rll}{\alpha x} & {\text { for }} & {x<0} \\ {x} & {\text { for }} & {x \geq 0}\end{array}\right.
```
当`α = 0.01`，==激活函数称为`Leaky ReLU`函数==。

`Leaky ReLU`函数如下图：

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/61e22de3ed9ed9bb6d733fedf7245dbc/90FE8BCD21674F99BA04D391BF44F712?ynotemdtimestamp=1565790366212" />
    <br/>
    <strong>Fig</strong>. Leaky Relu 函数
</p>

> `Leaky ReLU`函数解决了`ReLU`函数在**输入为负的情况下产生的梯度消失问题**。理论上虽然好于`ReLU`，但在实际使用中目前并没有好的证据`Leaky ReLU`总是优于`ReLU`。

### 8. `ELU`函数

指数线性单元（`xponential Linear Unit, ELU`）尝试**加快学习速度**。基于ELU，有可能得到比`ReLU`更高的分类精确度。这里`α`是一个超参数（`α ≥ 0`）。`ELU`函数（值域：`(-α, +∞)`）定义为：

```math
E L U=\left\{\begin{array}{c}{x, x>0} \\ {\alpha\left(e^{x}-1\right), x \leq 0}\end{array}\right.
```

`ELU`函数如下图：

<p align="center">
    <img src="https://note.youdao.com/yws/public/resource/61e22de3ed9ed9bb6d733fedf7245dbc/2E18BEB48B15421AA796552BC0B2BA2A?ynotemdtimestamp=1565791055710" width=600 />
    <br/>
    <strong>Fig</strong>. ELU 函数
</p>

> `ELU`函数的==优缺点==：
>
> 优点：
> 1. `ReLU`的基本所有优点
> 2. 不会有`Dead ReLU`问题
> 3. 输出的均值接近`0`，`zero-centered`
>
> 缺点：
>
> 1. 计算量稍大
> 2. 理论上虽然好于`ReLU`，但在实际使用中目前并没有好的证据`ELU`总是优于`ReLU`
